{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2cea34f-0b5c-4442-a59b-390cb4296181",
   "metadata": {},
   "source": [
    "Stage 1: Basic Feature Selection (Data Understanding & Initial Filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8a382ce-03b3-49f9-9f79-d4e64d00e749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: missingno in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.5.2)\n",
      "Requirement already satisfied: statsmodels in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.14.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from missingno) (2.2.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from missingno) (3.10.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from missingno) (1.15.3)\n",
      "Requirement already satisfied: seaborn in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from missingno) (0.13.2)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from statsmodels) (2.3.0)\n",
      "Requirement already satisfied: patsy>=0.5.6 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from statsmodels) (1.0.1)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from statsmodels) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->missingno) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->missingno) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->missingno) (4.58.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->missingno) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->missingno) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->missingno) (3.2.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: category_encoders in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from category_encoders) (2.2.0)\n",
      "Requirement already satisfied: pandas>=1.0.5 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from category_encoders) (2.3.0)\n",
      "Requirement already satisfied: patsy>=0.5.1 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from category_encoders) (1.0.1)\n",
      "Requirement already satisfied: scikit-learn>=1.6.0 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from category_encoders) (1.7.0)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from category_encoders) (1.15.3)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from category_encoders) (0.14.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.0.5->category_encoders) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=1.6.0->category_encoders) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=1.6.0->category_encoders) (3.6.0)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from statsmodels>=0.9.0->category_encoders) (25.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->category_encoders) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlxtend in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.23.4)\n",
      "Requirement already satisfied: scipy>=1.2.1 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlxtend) (1.15.3)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlxtend) (2.2.0)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlxtend) (2.3.0)\n",
      "Requirement already satisfied: scikit-learn>=1.3.1 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlxtend) (1.7.0)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlxtend) (3.10.3)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlxtend) (1.5.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (4.58.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2025.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=1.3.1->mlxtend) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from optuna) (1.16.2)\n",
      "Requirement already satisfied: colorlog in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from optuna) (2.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from optuna) (25.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from optuna) (2.0.41)\n",
      "Requirement already satisfied: tqdm in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: Mako in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.14.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyswarms in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyswarms) (1.15.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyswarms) (2.2.0)\n",
      "Requirement already satisfied: matplotlib>=1.3.1 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyswarms) (3.10.3)\n",
      "Requirement already satisfied: attrs in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyswarms) (25.3.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyswarms) (4.67.1)\n",
      "Requirement already satisfied: future in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyswarms) (1.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyswarms) (6.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=1.3.1->pyswarms) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=1.3.1->pyswarms) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=1.3.1->pyswarms) (4.58.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=1.3.1->pyswarms) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=1.3.1->pyswarms) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=1.3.1->pyswarms) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=1.3.1->pyswarms) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=1.3.1->pyswarms) (2.9.0.post0)\n",
      "Requirement already satisfied: colorama in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm->pyswarms) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=1.3.1->pyswarms) (1.17.0)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the name of the target column:  Survived\n"
     ]
    }
   ],
   "source": [
    "# 📦 Imports\n",
    "\n",
    "!pip install missingno statsmodels\n",
    "!pip install category_encoders\n",
    "!pip install mlxtend\n",
    "!pip install optuna\n",
    "!pip install pyswarms\n",
    "import missingno as msno\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import skew\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy.stats import skew, kurtosis, ttest_ind, chisquare\n",
    "\n",
    "# 📌 Load your dataset (Make sure it's uploaded in the same folder)\n",
    "\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "target_column = input(\"Enter the name of the target column: \")\n",
    "\n",
    "original_df = df.copy()\n",
    "y = df[target_column]\n",
    "df.drop(columns=[target_column], inplace=True)#Then removes it from the main feature set df to avoid data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3b373d5-617b-44d3-ba50-bcf857fbe9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Step 1: Data Profiling\n",
      "Shape of dataset: (891, 11)\n",
      "\n",
      "Feature types:\n",
      " PassengerId      int64\n",
      "Pclass           int64\n",
      "Name            object\n",
      "Sex             object\n",
      "Age            float64\n",
      "SibSp            int64\n",
      "Parch            int64\n",
      "Ticket          object\n",
      "Fare           float64\n",
      "Cabin           object\n",
      "Embarked        object\n",
      "dtype: object\n",
      "\n",
      "Summary statistics:\n",
      "         PassengerId      Pclass                 Name   Sex         Age  \\\n",
      "count    891.000000  891.000000                  891   891  714.000000   \n",
      "unique          NaN         NaN                  891     2         NaN   \n",
      "top             NaN         NaN  Dooley, Mr. Patrick  male         NaN   \n",
      "freq            NaN         NaN                    1   577         NaN   \n",
      "mean     446.000000    2.308642                  NaN   NaN   29.699118   \n",
      "std      257.353842    0.836071                  NaN   NaN   14.526497   \n",
      "min        1.000000    1.000000                  NaN   NaN    0.420000   \n",
      "25%      223.500000    2.000000                  NaN   NaN   20.125000   \n",
      "50%      446.000000    3.000000                  NaN   NaN   28.000000   \n",
      "75%      668.500000    3.000000                  NaN   NaN   38.000000   \n",
      "max      891.000000    3.000000                  NaN   NaN   80.000000   \n",
      "\n",
      "             SibSp       Parch  Ticket        Fare Cabin Embarked  \n",
      "count   891.000000  891.000000     891  891.000000   204      889  \n",
      "unique         NaN         NaN     681         NaN   147        3  \n",
      "top            NaN         NaN  347082         NaN    G6        S  \n",
      "freq           NaN         NaN       7         NaN     4      644  \n",
      "mean      0.523008    0.381594     NaN   32.204208   NaN      NaN  \n",
      "std       1.102743    0.806057     NaN   49.693429   NaN      NaN  \n",
      "min       0.000000    0.000000     NaN    0.000000   NaN      NaN  \n",
      "25%       0.000000    0.000000     NaN    7.910400   NaN      NaN  \n",
      "50%       0.000000    0.000000     NaN   14.454200   NaN      NaN  \n",
      "75%       1.000000    0.000000     NaN   31.000000   NaN      NaN  \n",
      "max       8.000000    6.000000     NaN  512.329200   NaN      NaN  \n",
      "\n",
      "Numerical_cols: ['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n",
      "Categorical_cols: ['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']\n",
      "\n",
      "📌 Column-wise Data Completeness (% of non-missing values):\n",
      "Cabin           22.90\n",
      "Age             80.13\n",
      "Embarked        99.78\n",
      "PassengerId    100.00\n",
      "Sex            100.00\n",
      "SibSp          100.00\n",
      "Name           100.00\n",
      "Pclass         100.00\n",
      "Ticket         100.00\n",
      "Parch          100.00\n",
      "Fare           100.00\n",
      "dtype: float64\n",
      "\n",
      "📌 Rows with <50% completeness: 0 / 891\n",
      "\n",
      "⚠️ Data Quality Issues Found:\n",
      " - Name: {'non-alphanumeric entries': np.int64(891)}\n",
      " - Ticket: {'non-alphanumeric entries': np.int64(150)}\n",
      "✅ Step 1 complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def step1_data_profiling(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Step 1: Data Profiling, Completeness, and Quality Check\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Same DataFrame (helper columns cleaned), and logs printed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"🔍 Step 1: Data Profiling\")\n",
    "\n",
    "        # Basic Structure\n",
    "        print(\"Shape of dataset:\", df.shape)\n",
    "        print(\"\\nFeature types:\\n\", df.dtypes)\n",
    "        print(\"\\nSummary statistics:\\n\", df.describe(include='all'))\n",
    "\n",
    "        # Type Classification\n",
    "        numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "        categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "        print(\"\\nNumerical_cols:\", numerical_cols)\n",
    "        print(\"Categorical_cols:\", categorical_cols)\n",
    "\n",
    "        # Column-wise completeness\n",
    "        completeness_col = df.notnull().mean().sort_values(ascending=True)\n",
    "        print(\"\\n📌 Column-wise Data Completeness (% of non-missing values):\")\n",
    "        print((completeness_col * 100).round(2))\n",
    "\n",
    "        # Row-wise completeness\n",
    "        df['row_completeness'] = df.notnull().mean(axis=1)\n",
    "        low_quality_rows = df[df['row_completeness'] < 0.5]\n",
    "        print(f\"\\n📌 Rows with <50% completeness: {len(low_quality_rows)} / {df.shape[0]}\")\n",
    "        df.drop(columns=['row_completeness'], inplace=True)\n",
    "\n",
    "        # Data Quality Issues Report\n",
    "        invalid_values_report = {}\n",
    "\n",
    "        # Check numerical columns\n",
    "        for col in numerical_cols:\n",
    "            if df[col].dtype in ['int64', 'float64']:\n",
    "                negatives = (df[col] < 0).sum()\n",
    "                infs = np.isinf(df[col]).sum()\n",
    "                nans = df[col].isna().sum()\n",
    "                if negatives > 0 or infs > 0:\n",
    "                    invalid_values_report[col] = {\n",
    "                        \"negatives\": negatives,\n",
    "                        \"infs\": infs,\n",
    "                        \"nulls\": nans\n",
    "                    }\n",
    "\n",
    "        # Check categorical columns\n",
    "        for col in categorical_cols:\n",
    "            non_alpha = df[col].astype(str).apply(\n",
    "                lambda x: not bool(re.match(\"^[a-zA-Z0-9 _-]*$\", x))\n",
    "            ).sum()\n",
    "            if non_alpha > 0:\n",
    "                invalid_values_report[col] = {\n",
    "                    \"non-alphanumeric entries\": non_alpha\n",
    "                }\n",
    "\n",
    "        # Print report\n",
    "        if invalid_values_report:\n",
    "            print(\"\\n⚠️ Data Quality Issues Found:\")\n",
    "            for col, issues in invalid_values_report.items():\n",
    "                print(f\" - {col}: {issues}\")\n",
    "        else:\n",
    "            print(\"\\n✅ No major data quality issues detected.\")\n",
    "\n",
    "        print(\"✅ Step 1 complete.\\n\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in Step 1 - Data Profiling: {e}\")\n",
    "        return df\n",
    "\n",
    "\n",
    "df = step1_data_profiling(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32b10be0-c09f-47b1-a614-5fb77c29bb85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                                               Name  \\\n",
       "0            1       3                            Braund, Mr. Owen Harris   \n",
       "1            2       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "2            3       3                             Heikkinen, Miss. Laina   \n",
       "3            4       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
       "4            5       3                           Allen, Mr. William Henry   \n",
       "\n",
       "      Sex   Age  SibSp  Parch            Ticket     Fare Cabin Embarked  \n",
       "0    male  22.0      1      0         A/5 21171   7.2500   NaN        S  \n",
       "1  female  38.0      1      0          PC 17599  71.2833   C85        C  \n",
       "2  female  26.0      0      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3  female  35.0      1      0            113803  53.1000  C123        S  \n",
       "4    male  35.0      0      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0b3d6fc-5c2f-45b6-8c93-7d38ca462181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Step 2: Missing Data Analysis\n",
      "\n",
      "📌 Missing Value Percentages:\n",
      "Cabin          77.10\n",
      "Age            19.87\n",
      "Embarked        0.22\n",
      "Name            0.00\n",
      "Pclass          0.00\n",
      "PassengerId     0.00\n",
      "Sex             0.00\n",
      "Parch           0.00\n",
      "SibSp           0.00\n",
      "Fare            0.00\n",
      "Ticket          0.00\n",
      "dtype: float64\n",
      "\n",
      "⚠️ Features with >40% missing values:\n",
      " - Cabin: 77.10%\n",
      "\n",
      "📊 Heatmap of missing data pattern...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Correlation Between Missing Patterns:\n",
      "✅ No strong correlations in missing patterns.\n",
      "\n",
      "🔎 Inferring Patterns of Missingness:\n",
      " - Age: Possibly MCAR or MNAR (no strong observed correlation)\n",
      " - Cabin: Likely MAR (correlated with: ['Pclass', 'Fare'])\n",
      " - Embarked: Possibly MCAR or MNAR (no strong observed correlation)\n",
      "✅ Step 2 complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def step2_missing_data_analysis(df: pd.DataFrame, threshold: float = 0.4) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Step 2: Analyze Missing Data Patterns, Severity, and Likely Mechanisms\n",
    "\n",
    "    -> Calculate missing value percentages per feature\n",
    "    -> Determine features with excessive missing values (>40-50% threshold)\n",
    "    -> Analyze missing value correlation patterns across features\n",
    "    -> Correlation between missing indicators to infer MCAR/MAR/MNAR\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        threshold (float): Proportion threshold to flag excessive missingness\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Same DataFrame with logging, no mutation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"🔍 Step 2: Missing Data Analysis\")\n",
    "\n",
    "        # 1. Percentage of missing data\n",
    "        missing_percent = df.isnull().mean().sort_values(ascending=False)\n",
    "        print(\"\\n📌 Missing Value Percentages:\")\n",
    "        print((missing_percent * 100).round(2))\n",
    "\n",
    "        # 2. Features above missing threshold\n",
    "        high_missing_cols = missing_percent[missing_percent > threshold].index.tolist()\n",
    "        if high_missing_cols:\n",
    "            print(f\"\\n⚠️ Features with >{int(threshold*100)}% missing values:\")\n",
    "            for col in high_missing_cols:\n",
    "                print(f\" - {col}: {(missing_percent[col] * 100):.2f}%\")\n",
    "        else:\n",
    "            print(f\"\\n✅ No features with >{int(threshold*100)}% missing values.\")\n",
    "\n",
    "        # 3. Visual Heatmap\n",
    "        print(\"\\n📊 Heatmap of missing data pattern...\")\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.heatmap(df.isnull(), cbar=False, yticklabels=False, cmap='viridis')\n",
    "        plt.title(\"Missing Data Heatmap\")\n",
    "        plt.show()\n",
    "\n",
    "        # 4. Correlation between missing indicators (null masks)\n",
    "        print(\"\\n📈 Correlation Between Missing Patterns:\")\n",
    "        missing_corr = df.isnull().astype(int).corr()\n",
    "        high_corr_pairs = [\n",
    "            (col1, col2, missing_corr.loc[col1, col2])\n",
    "            for col1 in missing_corr.columns\n",
    "            for col2 in missing_corr.columns\n",
    "            if col1 != col2 and abs(missing_corr.loc[col1, col2]) > 0.5\n",
    "        ]\n",
    "        if high_corr_pairs:\n",
    "            for col1, col2, val in high_corr_pairs:\n",
    "                print(f\" - {col1} vs {col2}: Correlation = {val:.2f}\")\n",
    "        else:\n",
    "            print(\"✅ No strong correlations in missing patterns.\")\n",
    "\n",
    "        # 5. Pattern Detection (MCAR / MAR / MNAR)\n",
    "        print(\"\\n🔎 Inferring Patterns of Missingness:\")\n",
    "        for col in df.columns[df.isnull().any()]:\n",
    "            indicator = df[col].isnull().astype(int)\n",
    "            temp = pd.concat([indicator, df.drop(columns=[col])], axis=1).dropna()\n",
    "\n",
    "            # 🛠️ Filter to only numeric columns for correlation\n",
    "            temp_numeric = temp.select_dtypes(include=['number'])\n",
    "            if indicator.name not in temp_numeric.columns:\n",
    "                temp_numeric[indicator.name] = indicator\n",
    "\n",
    "            correlations = temp_numeric.corr()[indicator.name].drop(indicator.name).abs().sort_values(ascending=False)\n",
    "            top_corr = correlations[correlations > 0.3]\n",
    "            if not top_corr.empty:\n",
    "                print(f\" - {col}: Likely MAR (correlated with: {list(top_corr.index[:3])})\")\n",
    "            else:\n",
    "                print(f\" - {col}: Possibly MCAR or MNAR (no strong observed correlation)\")\n",
    "\n",
    "        print(\"✅ Step 2 complete.\\n\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in Step 2 - Missing Data Analysis: {e}\")\n",
    "        return df\n",
    "\n",
    "df = step2_missing_data_analysis(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42b7afed-6f07-4cd2-a1de-d0073aef2210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Step 3: Cardinality and Distribution\n",
      "Name → Unique Values: 891\n",
      "Sex → Unique Values: 2\n",
      "Ticket → Unique Values: 681\n",
      "Cabin → Unique Values: 147\n",
      "Embarked → Unique Values: 3\n",
      "PassengerId → Skewness: 0.00, Kurtosis: -1.20\n",
      "Pclass → Skewness: -0.63, Kurtosis: -1.28\n",
      "Age → Skewness: 0.39, Kurtosis: 0.17\n",
      "SibSp → Skewness: 3.69, Kurtosis: 17.77\n",
      "Parch → Skewness: 2.74, Kurtosis: 9.72\n",
      "Fare → Skewness: 4.78, Kurtosis: 33.20\n",
      "\n",
      "⚠️ High-Cardinality Categorical Features Detected:\n",
      " - Name (891 unique values)\n",
      " - Ticket (681 unique values)\n",
      " - Cabin (147 unique values)\n",
      "\n",
      "🧮 Detecting Constant/Quasi-Constant Features...\n",
      "✅ No constant or quasi-constant features found.\n",
      "✅ Step 3 complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def step3_cardinality_and_distribution(df: pd.DataFrame,\n",
    "                                       numerical_cols: list,\n",
    "                                       categorical_cols: list,\n",
    "                                       variance_threshold: float = 0.01) -> tuple:\n",
    "    \"\"\"\n",
    "    Step 3: Cardinality, Distribution, and Low Variance Feature Detection\n",
    "\n",
    "    -> Print unique counts in categorical features and flag high cardinality\n",
    "    -> Print skewness and kurtosis of numerical features\n",
    "    -> Detect constant and quasi-constant numerical features using variance threshold\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        numerical_cols (list): List of numerical column names\n",
    "        categorical_cols (list): List of categorical column names\n",
    "        variance_threshold (float): Threshold for VarianceThreshold\n",
    "\n",
    "    Returns:\n",
    "        tuple: (Updated DataFrame, updated numerical_cols list, categorical_cols list)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"\\n🔍 Step 3: Cardinality and Distribution\")\n",
    "\n",
    "        high_cardinality_features = []\n",
    "\n",
    "        # 1. Categorical column analysis\n",
    "        for col in categorical_cols:\n",
    "            unique_vals = df[col].nunique()\n",
    "            print(f\"{col} → Unique Values: {unique_vals}\")\n",
    "            if unique_vals > 50:\n",
    "                high_cardinality_features.append(col)\n",
    "\n",
    "        # 2. Numerical column distribution analysis\n",
    "        for col in numerical_cols:\n",
    "            skew_val = skew(df[col].dropna())\n",
    "            kurt_val = kurtosis(df[col].dropna())\n",
    "            print(f\"{col} → Skewness: {skew_val:.2f}, Kurtosis: {kurt_val:.2f}\")\n",
    "\n",
    "        # 3. Report high-cardinality categorical features\n",
    "        if high_cardinality_features:\n",
    "            print(\"\\n⚠️ High-Cardinality Categorical Features Detected:\")\n",
    "            for col in high_cardinality_features:\n",
    "                print(f\" - {col} ({df[col].nunique()} unique values)\")\n",
    "        else:\n",
    "            print(\"\\n✅ No high-cardinality categorical features detected.\")\n",
    "\n",
    "        # 4. Variance Threshold: Constant and Quasi-Constant Feature Detection\n",
    "        print(\"\\n🧮 Detecting Constant/Quasi-Constant Features...\")\n",
    "        selector = VarianceThreshold(threshold=variance_threshold)\n",
    "        selector.fit(df[numerical_cols])\n",
    "\n",
    "        low_variance_cols = [col for col, var in zip(numerical_cols, selector.variances_) if var < variance_threshold]\n",
    "\n",
    "        if low_variance_cols:\n",
    "            print(\"⚠️ Constant/Quasi-Constant Features Detected:\")\n",
    "            for col in low_variance_cols:\n",
    "                print(f\" - {col} (Variance: {np.var(df[col]):.6f})\")\n",
    "\n",
    "            df.drop(columns=low_variance_cols, inplace=True)\n",
    "            numerical_cols = [col for col in numerical_cols if col not in low_variance_cols]\n",
    "        else:\n",
    "            print(\"✅ No constant or quasi-constant features found.\")\n",
    "\n",
    "        print(\"✅ Step 3 complete.\\n\")\n",
    "        return df, numerical_cols, categorical_cols\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in Step 3 - Cardinality and Distribution: {e}\")\n",
    "        return df, numerical_cols, categorical_cols\n",
    "\n",
    "\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "     \n",
    "df, numerical_cols, categorical_cols = step3_cardinality_and_distribution(df, numerical_cols, categorical_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2b775a5-20a9-41c8-84e3-6dd0c7493f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧹 Step 4: Constant and Quasi-Constant Feature Removal\n",
      "✅ No constant or low-variance features found.\n",
      "✅ Step 4 complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def step4_low_variance_removal(df: pd.DataFrame, numerical_cols: list,\n",
    "                                quasi_constant_threshold: float = 0.95,\n",
    "                                low_variance_threshold: float = 1e-4) -> tuple:\n",
    "    \"\"\"\n",
    "    Step 4: Remove features with zero variance (constant values)\n",
    "            Remove quasi-constant features (>95% same value)\n",
    "            Eliminate features with extremely low variance relative to domain context\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        numerical_cols (list): List of numerical columns\n",
    "        quasi_constant_threshold (float): Threshold for quasi-constant ratio (default = 0.95)\n",
    "        low_variance_threshold (float): Threshold for extremely low variance (default = 0.0001)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (Updated DataFrame, updated numerical_cols list)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"\\n🧹 Step 4: Constant and Quasi-Constant Feature Removal\")\n",
    "\n",
    "        low_variance_features = []\n",
    "        quasi_constant_features = []\n",
    "        extremely_low_variance = []\n",
    "\n",
    "        for col in numerical_cols:\n",
    "            col_series = df[col].dropna()\n",
    "\n",
    "            # 1️⃣ Constant Features\n",
    "            if col_series.nunique() == 1:\n",
    "                low_variance_features.append(col)\n",
    "                continue\n",
    "\n",
    "            # 2️⃣ Quasi-Constant: >95% same value\n",
    "            top_freq_ratio = col_series.value_counts(normalize=True).values[0]\n",
    "            if top_freq_ratio > quasi_constant_threshold:\n",
    "                quasi_constant_features.append(col)\n",
    "                continue\n",
    "\n",
    "            # 3️⃣ Extremely Low Variance\n",
    "            col_variance = col_series.var()\n",
    "            if col_variance < low_variance_threshold:\n",
    "                extremely_low_variance.append(col)\n",
    "\n",
    "        # Combine and remove\n",
    "        to_remove = list(set(low_variance_features + quasi_constant_features + extremely_low_variance))\n",
    "        df.drop(columns=to_remove, inplace=True)\n",
    "\n",
    "        # Update numerical columns\n",
    "        numerical_cols = [col for col in numerical_cols if col not in to_remove]\n",
    "\n",
    "        # Reporting\n",
    "        if to_remove:\n",
    "            print(f\"⚠️ Removed {len(to_remove)} low-variance features:\")\n",
    "            if low_variance_features:\n",
    "                print(f\" - Constant Features: {low_variance_features}\")\n",
    "            if quasi_constant_features:\n",
    "                print(f\" - Quasi-Constant Features: {quasi_constant_features}\")\n",
    "            if extremely_low_variance:\n",
    "                print(f\" - Extremely Low Variance Features: {extremely_low_variance}\")\n",
    "        else:\n",
    "            print(\"✅ No constant or low-variance features found.\")\n",
    "\n",
    "        print(\"✅ Step 4 complete.\\n\")\n",
    "        return df, numerical_cols\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in Step 4 - Low Variance Removal: {e}\")\n",
    "        return df, numerical_cols\n",
    "\n",
    "\n",
    "        \n",
    "df, numerical_cols = step4_low_variance_removal(df, numerical_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3742d21-260f-40ec-83f7-1c207e010ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Removed 1 features with >40% missing values:\n",
      " - ['Cabin']\n",
      "ℹ️ Threshold of 40% was applied.\n"
     ]
    }
   ],
   "source": [
    "def step5_remove_high_missing_features(df, numerical_cols, categorical_cols, threshold=0.4, critical_features=None):\n",
    "    \"\"\"\n",
    "    ->Remove features with missing values exceeding business tolerance (typically >30-50%)\n",
    "    ->Document rationale for threshold selection based on dataset size and domain requirements\n",
    "    ->Consider feature importance before elimination if domain-critical\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input dataframe.\n",
    "        numerical_cols (list): List of numerical feature names.\n",
    "        categorical_cols (list): List of categorical feature names.\n",
    "        threshold (float): Threshold percentage (0-1) for missing value removal.\n",
    "        critical_features (list): Optional list of domain-critical features to retain.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with high-missing features removed.\n",
    "        list: Updated numerical column list.\n",
    "        list: Updated categorical column list.\n",
    "    \"\"\"\n",
    "    if critical_features is None:\n",
    "        critical_features = []\n",
    "\n",
    "    # Calculate missing percentage\n",
    "    missing_report = df.isnull().mean().sort_values(ascending=False)\n",
    "    high_missing_cols = missing_report[missing_report > threshold].index.tolist()\n",
    "\n",
    "    # Retain domain-critical features\n",
    "    retained = [col for col in high_missing_cols if col in critical_features]\n",
    "    high_missing_cols = [col for col in high_missing_cols if col not in critical_features]\n",
    "\n",
    "    # Drop features with excessive missing values\n",
    "    df.drop(columns=high_missing_cols, inplace=True)\n",
    "\n",
    "    # Update numerical and categorical columns\n",
    "    numerical_cols = [col for col in numerical_cols if col not in high_missing_cols]\n",
    "    categorical_cols = [col for col in categorical_cols if col not in high_missing_cols]\n",
    "\n",
    "    # ✅ Report\n",
    "    if high_missing_cols:\n",
    "        print(f\"⚠️ Removed {len(high_missing_cols)} features with >{int(threshold * 100)}% missing values:\")\n",
    "        print(\" -\", high_missing_cols)\n",
    "        if retained:\n",
    "            print(f\"🔐 Retained domain-critical features despite high missingness: {retained}\")\n",
    "        print(f\"ℹ️ Threshold of {int(threshold * 100)}% was applied.\")\n",
    "    else:\n",
    "        print(\"✅ No features with excessive missing values found.\")\n",
    "\n",
    "    return df, numerical_cols, categorical_cols\n",
    "\n",
    "\n",
    "df, numerical_cols, categorical_cols = step5_remove_high_missing_features(\n",
    "    df=df,\n",
    "    numerical_cols=numerical_cols,\n",
    "    categorical_cols=categorical_cols,\n",
    "    threshold=0.4,\n",
    "    critical_features=['customer_income']  # Optional\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f28c4b3-5f09-4cae-b385-8b5b84ca8706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                                               Name  \\\n",
       "0            1       3                            Braund, Mr. Owen Harris   \n",
       "1            2       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "2            3       3                             Heikkinen, Miss. Laina   \n",
       "3            4       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
       "4            5       3                           Allen, Mr. William Henry   \n",
       "\n",
       "      Sex   Age  SibSp  Parch            Ticket     Fare Embarked  \n",
       "0    male  22.0      1      0         A/5 21171   7.2500        S  \n",
       "1  female  38.0      1      0          PC 17599  71.2833        C  \n",
       "2  female  26.0      0      0  STON/O2. 3101282   7.9250        S  \n",
       "3  female  35.0      1      0            113803  53.1000        S  \n",
       "4    male  35.0      0      0            373450   8.0500        S  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e00d738-4aa0-49f4-8abe-1892feae193b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ No redundant or duplicate features found.\n"
     ]
    }
   ],
   "source": [
    "def Step6_remove_redundant_features(df, numerical_cols, categorical_cols):\n",
    "    \"\"\"\n",
    "     ->Identify perfectly correlated features (correlation = 1.0)\n",
    "     ->Detect duplicate columns with different names\n",
    "     ->Remove one feature from each duplicate pair, prioritizing business interpretability\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input dataframe.\n",
    "        numerical_cols (list): List of numerical column names.\n",
    "        categorical_cols (list): List of categorical column names.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned DataFrame.\n",
    "        list: Updated numerical columns.\n",
    "        list: Updated categorical columns.\n",
    "    \"\"\"\n",
    "    redundant_cols = set()\n",
    "\n",
    "    # 1️⃣ Perfectly correlated numerical features\n",
    "    corr_matrix = df.select_dtypes(include=[np.number]).corr()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "    for col in upper.columns:\n",
    "        for row in upper.index:\n",
    "            if pd.notnull(upper.loc[row, col]) and upper.loc[row, col] == 1.0:\n",
    "                redundant_cols.add(col)  # Keep one of the pair\n",
    "\n",
    "    # 2️⃣ Duplicate columns (identical values across all rows)\n",
    "    cols = df.columns\n",
    "    for i in range(len(cols)):\n",
    "        for j in range(i + 1, len(cols)):\n",
    "            if cols[j] not in redundant_cols and df[cols[i]].equals(df[cols[j]]):\n",
    "                redundant_cols.add(cols[j])  # Keep the first occurrence\n",
    "\n",
    "    # Drop redundant columns\n",
    "    df.drop(columns=list(redundant_cols), inplace=True)\n",
    "\n",
    "    # Update column trackers\n",
    "    numerical_cols = [col for col in numerical_cols if col not in redundant_cols]\n",
    "    categorical_cols = [col for col in categorical_cols if col not in redundant_cols]\n",
    "\n",
    "    # 🧾 Reporting\n",
    "    if redundant_cols:\n",
    "        print(f\"⚠️ Removed {len(redundant_cols)} redundant features:\")\n",
    "        print(\" -\", list(redundant_cols))\n",
    "        print(\"ℹ️ Perfectly correlated features (corr = 1.0) and duplicate columns were dropped.\")\n",
    "    else:\n",
    "        print(\"✅ No redundant or duplicate features found.\")\n",
    "\n",
    "    return df, numerical_cols, categorical_cols\n",
    "\n",
    "df, numerical_cols, categorical_cols = Step6_remove_redundant_features(\n",
    "    df=df,\n",
    "    numerical_cols=numerical_cols,\n",
    "    categorical_cols=categorical_cols\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95d364b4-c5b6-40dc-a59b-3b80ba9b836f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                                               Name  \\\n",
       "0            1       3                            Braund, Mr. Owen Harris   \n",
       "1            2       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "2            3       3                             Heikkinen, Miss. Laina   \n",
       "3            4       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
       "4            5       3                           Allen, Mr. William Henry   \n",
       "\n",
       "      Sex   Age  SibSp  Parch            Ticket     Fare Embarked  \n",
       "0    male  22.0      1      0         A/5 21171   7.2500        S  \n",
       "1  female  38.0      1      0          PC 17599  71.2833        C  \n",
       "2  female  26.0      0      0  STON/O2. 3101282   7.9250        S  \n",
       "3  female  35.0      1      0            113803  53.1000        S  \n",
       "4    male  35.0      0      0            373450   8.0500        S  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "17c4ae6e-7039-45f1-81a8-12e3ead90ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             PassengerId    Pclass       Age     SibSp     Parch      Fare\n",
      "PassengerId     1.000000  0.035144  0.036847  0.057527  0.001652  0.012658\n",
      "Pclass          0.035144  1.000000  0.369226  0.083081  0.018443  0.549500\n",
      "Age             0.036847  0.369226  1.000000  0.308247  0.189119  0.096067\n",
      "SibSp           0.057527  0.083081  0.308247  1.000000  0.414838  0.159651\n",
      "Parch           0.001652  0.018443  0.189119  0.414838  1.000000  0.216225\n",
      "Fare            0.012658  0.549500  0.096067  0.159651  0.216225  1.000000\n",
      "\n",
      "✅ No highly correlated feature pairs found.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def Step7_identify_highly_correlated_features(df, threshold=0.9):\n",
    "    \"\"\"\n",
    "    ->Calculate pairwise correlation matrix for numerical features\n",
    "    ->Identify feature pairs with correlation >0.8-0.95 (threshold depends on domain)\n",
    "    ->Create correlation clusters for further analysis\n",
    "    ->Document highly correlated feature groups for later engineering decisions\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame (should include only relevant features).\n",
    "        threshold (float): Correlation coefficient threshold (e.g., 0.9).\n",
    "    \n",
    "    Returns:\n",
    "        list of tuples: Each tuple contains (feature1, feature2, correlation).\n",
    "        list of sets: Clusters of correlated features.\n",
    "    \"\"\"\n",
    "    # Only numeric features\n",
    "    num_df = df.select_dtypes(include=[np.number])\n",
    "    corr_matrix = num_df.corr().abs()\n",
    "    print(corr_matrix)\n",
    "\n",
    "    # Upper triangle of the correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "    # Extract pairs with correlation > threshold\n",
    "    high_corr_pairs = [\n",
    "        (col1, col2, upper.loc[col1, col2])\n",
    "        for col1 in upper.columns\n",
    "        for col2 in upper.index\n",
    "        if pd.notna(upper.loc[col1, col2]) and upper.loc[col1, col2] > threshold\n",
    "    ]\n",
    "\n",
    "    # Build correlation clusters\n",
    "    clusters = defaultdict(set)\n",
    "    for col1, col2, _ in high_corr_pairs:\n",
    "        clusters[col1].update([col1, col2])\n",
    "        clusters[col2].update([col1, col2])\n",
    "\n",
    "    # Merge overlapping clusters\n",
    "    def merge_clusters(cluster_dict):\n",
    "        cluster_sets = list(cluster_dict.values())\n",
    "        merged_clusters = []\n",
    "        while cluster_sets:\n",
    "            first, *rest = cluster_sets\n",
    "            first = set(first)\n",
    "            changed = True\n",
    "            while changed:\n",
    "                changed = False\n",
    "                rest2 = []\n",
    "                for r in rest:\n",
    "                    if first & r:\n",
    "                        first |= r\n",
    "                        changed = True\n",
    "                    else:\n",
    "                        rest2.append(r)\n",
    "                rest = rest2\n",
    "            merged_clusters.append(first)\n",
    "            cluster_sets = rest\n",
    "        return merged_clusters\n",
    "\n",
    "    correlation_clusters = merge_clusters(clusters)\n",
    "\n",
    "    # 🔍 Report\n",
    "    if high_corr_pairs:\n",
    "        print(f\"\\n⚠️ Found {len(high_corr_pairs)} highly correlated feature pairs (r > {threshold}):\")\n",
    "        for col1, col2, corr in high_corr_pairs:\n",
    "            print(f\" - {col1} & {col2} → correlation: {corr:.2f}\")\n",
    "        print(\"\\n📊 Correlation Clusters Identified:\")\n",
    "        for i, group in enumerate(correlation_clusters, 1):\n",
    "            print(f\"  Cluster {i}: {sorted(group)}\")\n",
    "    else:\n",
    "        print(\"\\n✅ No highly correlated feature pairs found.\")\n",
    "\n",
    "    return high_corr_pairs, correlation_clusters\n",
    "\n",
    "    \n",
    "high_corr_pairs, correlation_clusters = Step7_identify_highly_correlated_features(df, threshold=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "86b59be3-4ac4-4536-b033-09a08846332a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                                               Name  \\\n",
       "0            1       3                            Braund, Mr. Owen Harris   \n",
       "1            2       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "2            3       3                             Heikkinen, Miss. Laina   \n",
       "3            4       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
       "4            5       3                           Allen, Mr. William Henry   \n",
       "\n",
       "      Sex   Age  SibSp  Parch            Ticket     Fare Embarked  \n",
       "0    male  22.0      1      0         A/5 21171   7.2500        S  \n",
       "1  female  38.0      1      0          PC 17599  71.2833        C  \n",
       "2  female  26.0      0      0  STON/O2. 3101282   7.9250        S  \n",
       "3  female  35.0      1      0            113803  53.1000        S  \n",
       "4    male  35.0      0      0            373450   8.0500        S  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6512a6a-7082-4ee9-9d15-9876054e1bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the significance level alpha (between 0 and 1, default=0.05):  0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Statistical Test Summary:\n",
      "       feature       p_value   test  bonferroni_corrected  fdr_corrected\n",
      "1          Sex  1.197357e-58   chi2                  True           True\n",
      "5       Pclass  2.537047e-25  ttest                  True           True\n",
      "9         Fare  6.120189e-15  ttest                  True           True\n",
      "3     Embarked  1.769922e-06   chi2                  True           True\n",
      "2       Ticket  1.152730e-02   chi2                 False           True\n",
      "8        Parch  1.479925e-02  ttest                 False           True\n",
      "6          Age  3.912465e-02  ttest                 False          False\n",
      "7        SibSp  2.922439e-01  ttest                 False          False\n",
      "0         Name  4.842482e-01   chi2                 False          False\n",
      "4  PassengerId  8.813658e-01  ttest                 False          False\n",
      "\n",
      "✅ Kept 7 significant features (p < 0.05)\n",
      "❌ Removed 3 non-significant features: ['SibSp', 'Name', 'PassengerId']\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency, ttest_ind, f_oneway\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "\n",
    "def get_significance_level(default_alpha=0.05):\n",
    "    \"\"\"\n",
    "    Prompts user to enter a significance level alpha (between 0 and 1).\n",
    "    Falls back to default_alpha if invalid input is provided.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        alpha = float(input(f\"Enter the significance level alpha (between 0 and 1, default={default_alpha}): \"))\n",
    "        if not 0 < alpha < 1:\n",
    "            raise ValueError\n",
    "    except ValueError:\n",
    "        print(f\"⚠️ Invalid input. Defaulting to alpha = {default_alpha}\")\n",
    "        alpha = default_alpha\n",
    "    return alpha\n",
    "\n",
    "def univariate_statistical_testing(df, y, numerical_cols, categorical_cols, alpha):\n",
    "    \"\"\"\n",
    "    Performs univariate statistical significance testing for each feature with respect to the target variable.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input dataframe (features only).\n",
    "        y (pd.Series): Target variable.\n",
    "        numerical_cols (list): List of numerical feature names.\n",
    "        categorical_cols (list): List of categorical feature names.\n",
    "        alpha (float): Significance level threshold.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: P-value dataframe with test types and corrections.\n",
    "        pd.DataFrame: Updated dataframe with only statistically significant features.\n",
    "    \"\"\"\n",
    "    p_values = {}\n",
    "    method_used = {}\n",
    "\n",
    "    # ➤ Chi-Square Test for Categorical Features\n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns and df[col].nunique() > 1 and y.nunique() > 1:\n",
    "            try:\n",
    "                contingency_table = pd.crosstab(df[col], y)\n",
    "                _, p, _, _ = chi2_contingency(contingency_table)\n",
    "                p_values[col] = p\n",
    "                method_used[col] = 'chi2'\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    # ➤ T-Test or ANOVA for Numerical Features\n",
    "    for col in numerical_cols:\n",
    "        if col in df.columns and df[col].nunique() > 1:\n",
    "            try:\n",
    "                groups = [df[col][y == label].dropna() for label in y.unique()]\n",
    "                if len(groups) == 2:\n",
    "                    _, p = ttest_ind(*groups)\n",
    "                    method_used[col] = 'ttest'\n",
    "                elif len(groups) > 2:\n",
    "                    _, p = f_oneway(*groups)\n",
    "                    method_used[col] = 'anova'\n",
    "                else:\n",
    "                    continue\n",
    "                p_values[col] = p\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    # ➤ Store P-values and Methods\n",
    "    pval_df = pd.DataFrame({\n",
    "        \"feature\": list(p_values.keys()),\n",
    "        \"p_value\": list(p_values.values()),\n",
    "        \"test\": [method_used[f] for f in p_values.keys()]\n",
    "    })\n",
    "\n",
    "    # ➤ Multiple Testing Correction\n",
    "    pval_df[\"bonferroni_corrected\"] = multipletests(pval_df[\"p_value\"], method='bonferroni')[0]\n",
    "    pval_df[\"fdr_corrected\"] = multipletests(pval_df[\"p_value\"], method='fdr_bh')[0]\n",
    "\n",
    "    # ➤ Filter significant features\n",
    "    significant_features = pval_df[pval_df[\"p_value\"] < alpha][\"feature\"].tolist()\n",
    "    removed_features = list(set(df.columns) - set(significant_features))\n",
    "    df_filtered = df[significant_features]\n",
    "\n",
    "    # 🧾 Summary Report\n",
    "    print(\"\\n📊 Statistical Test Summary:\")\n",
    "    print(pval_df.sort_values(\"p_value\"))\n",
    "\n",
    "    print(f\"\\n✅ Kept {len(significant_features)} significant features (p < {alpha})\")\n",
    "    print(f\"❌ Removed {len(removed_features)} non-significant features:\", removed_features)\n",
    "\n",
    "    return pval_df, df_filtered\n",
    "\n",
    "alpha = get_significance_level()\n",
    "pval_summary_df, df = univariate_statistical_testing(df, y, numerical_cols, categorical_cols, alpha)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "705cd8cf-1293-41a0-9112-abfbdd0f0351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>female</td>\n",
       "      <td>113803</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male</td>\n",
       "      <td>373450</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sex            Ticket Embarked  Pclass   Age  Parch     Fare\n",
       "0    male         A/5 21171        S       3  22.0      0   7.2500\n",
       "1  female          PC 17599        C       1  38.0      0  71.2833\n",
       "2  female  STON/O2. 3101282        S       3  26.0      0   7.9250\n",
       "3  female            113803        S       1  35.0      0  53.1000\n",
       "4    male            373450        S       3  35.0      0   8.0500"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb513f1-4342-4f6c-ac44-247ecc594950",
   "metadata": {},
   "source": [
    "Stage 2: Data and Feature Engineering (Creation & Transformation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6feaeaa3-fe15-430b-b67d-812ac95b230c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Step 9: Missing Value Imputation Strategy\n",
      "\n",
      "🔍 Step 9.1: Creating Missing Value Indicators\n",
      "🆕 Added indicator columns for: ['Embarked_missing', 'Age_missing']\n",
      "\n",
      "🔧 Step 9.2: Applying Simple Imputation\n",
      "✅ Age: mean imputation\n",
      "✅ Embarked: mode imputation\n",
      "\n",
      "⏩ Step 9.3: Time-based Fill for Datetime Columns\n",
      "ℹ️ No datetime columns found. Skipping forward/backward fill.\n",
      "\n",
      "🧠 Step 9.4: KNN Imputation (if required)\n",
      "✅ No missing values remaining. KNN imputation not needed.\n",
      "\n",
      "🔁 Step 9.5: Multiple Imputation (Advanced - Placeholder)\n",
      "ℹ️ Consider using sklearn.experimental.IterativeImputer or fancyimpute for advanced use cases\n",
      "\n",
      "✅ All missing values successfully imputed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked_missing</th>\n",
       "      <th>Age_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>female</td>\n",
       "      <td>113803</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male</td>\n",
       "      <td>373450</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sex            Ticket Embarked  Pclass   Age  Parch     Fare  \\\n",
       "0    male         A/5 21171        S       3  22.0      0   7.2500   \n",
       "1  female          PC 17599        C       1  38.0      0  71.2833   \n",
       "2  female  STON/O2. 3101282        S       3  26.0      0   7.9250   \n",
       "3  female            113803        S       1  35.0      0  53.1000   \n",
       "4    male            373450        S       3  35.0      0   8.0500   \n",
       "\n",
       "   Embarked_missing  Age_missing  \n",
       "0                 0            0  \n",
       "1                 0            0  \n",
       "2                 0            0  \n",
       "3                 0            0  \n",
       "4                 0            0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def step9_missing_value_imputation(df):\n",
    "    \"\"\"\n",
    "    Modular function to perform full missing value imputation strategy:\n",
    "    -> Add missing value indicators\n",
    "    -> Apply mean/median/mode imputation\n",
    "    -> Forward/backward fill for datetime columns\n",
    "    -> KNN imputation if any missing values remain\n",
    "    -> Placeholder for advanced multiple imputation\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Imputed DataFrame\n",
    "    \"\"\"\n",
    "    print(\"\\n🔍 Step 9: Missing Value Imputation Strategy\")\n",
    "\n",
    "    # Separate column types\n",
    "    numerical_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n",
    "    datetime_cols = df.select_dtypes(include=['datetime64[ns]']).columns.tolist()\n",
    "\n",
    "    # 🔍 Step 9.1: Create Missing Value Indicator Features\n",
    "    print(\"\\n🔍 Step 9.1: Creating Missing Value Indicators\")\n",
    "    missing_indicators = []\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            indicator_name = f\"{col}_missing\"\n",
    "            df[indicator_name] = df[col].isnull().astype(int)\n",
    "            missing_indicators.append(indicator_name)\n",
    "    print(\"🆕 Added indicator columns for:\", missing_indicators if missing_indicators else \"None\")\n",
    "\n",
    "    # 🔧 Step 9.2: Simple Imputation (Mean/Median for numerical, Mode for categorical)\n",
    "    print(\"\\n🔧 Step 9.2: Applying Simple Imputation\")\n",
    "\n",
    "    if numerical_cols:\n",
    "        for col in numerical_cols:\n",
    "            if df[col].isnull().sum() > 0:\n",
    "                strategy = 'mean' if abs(skew(df[col].dropna())) < 1 else 'median'\n",
    "                fill_value = df[col].mean() if strategy == 'mean' else df[col].median()\n",
    "                df[col].fillna(fill_value, inplace=True)\n",
    "                print(f\"✅ {col}: {strategy} imputation\")\n",
    "    else:\n",
    "        print(\"ℹ️ No numerical columns found. Skipping numerical imputation.\")\n",
    "\n",
    "    if categorical_cols:\n",
    "        for col in categorical_cols:\n",
    "            if df[col].isnull().sum() > 0:\n",
    "                mode_val = df[col].mode()[0]\n",
    "                df[col].fillna(mode_val, inplace=True)\n",
    "                print(f\"✅ {col}: mode imputation\")\n",
    "    else:\n",
    "        print(\"ℹ️ No categorical columns found. Skipping categorical imputation.\")\n",
    "\n",
    "    # ⏩ Step 9.3: Forward/Backward Fill for Datetime Columns\n",
    "    print(\"\\n⏩ Step 9.3: Time-based Fill for Datetime Columns\")\n",
    "    if datetime_cols:\n",
    "        for col in datetime_cols:\n",
    "            df.sort_values(by=col, inplace=True)\n",
    "            df.fillna(method='ffill', inplace=True)\n",
    "            df.fillna(method='bfill', inplace=True)\n",
    "            print(f\"✅ {col}: forward/backward filled\")\n",
    "    else:\n",
    "        print(\"ℹ️ No datetime columns found. Skipping forward/backward fill.\")\n",
    "\n",
    "    # 🧠 Step 9.4: KNN Imputation if still missing values exist\n",
    "    print(\"\\n🧠 Step 9.4: KNN Imputation (if required)\")\n",
    "    if df.isnull().sum().sum() > 0:\n",
    "        print(\"⏳ KNN Imputer running...\")\n",
    "        temp_df = df.copy()\n",
    "\n",
    "        # Only encode if categorical columns exist\n",
    "        if categorical_cols:\n",
    "            for col in categorical_cols:\n",
    "                temp_df[col] = LabelEncoder().fit_transform(temp_df[col].astype(str))\n",
    "\n",
    "        knn_imputer = KNNImputer(n_neighbors=5)\n",
    "        imputed_array = knn_imputer.fit_transform(temp_df)\n",
    "        df = pd.DataFrame(imputed_array, columns=temp_df.columns)\n",
    "        print(\"✅ KNN Imputation complete\")\n",
    "    else:\n",
    "        print(\"✅ No missing values remaining. KNN imputation not needed.\")\n",
    "\n",
    "    # 🔁 Step 9.5: Placeholder for Multiple Imputation\n",
    "    print(\"\\n🔁 Step 9.5: Multiple Imputation (Advanced - Placeholder)\")\n",
    "    print(\"ℹ️ Consider using sklearn.experimental.IterativeImputer or fancyimpute for advanced use cases\")\n",
    "\n",
    "    # ✅ Final check\n",
    "    if df.isnull().sum().sum() == 0:\n",
    "        print(\"\\n✅ All missing values successfully imputed.\")\n",
    "    else:\n",
    "        print(\"\\n⚠️ Some missing values could not be imputed. Manual review may be required.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = step9_missing_value_imputation(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2e5d16b7-38a3-44cf-b8e6-deb30195e651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧩 Imputation Report:\n",
      " - Age: Numerical → Mean Imputation\n",
      " - Embarked: Categorical → Mode Imputation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked_missing</th>\n",
       "      <th>Age_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>female</td>\n",
       "      <td>113803</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male</td>\n",
       "      <td>373450</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sex            Ticket Embarked  Pclass   Age  Parch     Fare  \\\n",
       "0    male         A/5 21171        S       3  22.0      0   7.2500   \n",
       "1  female          PC 17599        C       1  38.0      0  71.2833   \n",
       "2  female  STON/O2. 3101282        S       3  26.0      0   7.9250   \n",
       "3  female            113803        S       1  35.0      0  53.1000   \n",
       "4    male            373450        S       3  35.0      0   8.0500   \n",
       "\n",
       "   Embarked_missing  Age_missing  \n",
       "0                 0            0  \n",
       "1                 0            0  \n",
       "2                 0            0  \n",
       "3                 0            0  \n",
       "4                 0            0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def generate_imputation_report(original_df, df, numerical_cols, categorical_cols, datetime_cols=None):\n",
    "    \"\"\"\n",
    "    ->Generates a report summarizing which imputation strategy was used per column\n",
    "    ->based on comparison between original and imputed DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "        original_df (pd.DataFrame): Original DataFrame (before imputation).\n",
    "        df (pd.DataFrame): DataFrame after imputation.\n",
    "        numerical_cols (list): List of numerical columns.\n",
    "        categorical_cols (list): List of categorical columns.\n",
    "        datetime_cols (list or None): List of datetime columns. Optional.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of columns and their respective imputation strategies.\n",
    "    \"\"\"\n",
    "    imputation_report = {}\n",
    "\n",
    "    # ✅ Handle numerical columns\n",
    "    for col in numerical_cols:\n",
    "        if col in original_df.columns and col in df.columns:\n",
    "            if original_df[col].isnull().sum() > 0 and df[col].isnull().sum() == 0:\n",
    "                orig_mean = original_df[col].mean()\n",
    "                new_mean = df[col].mean()\n",
    "                if np.isfinite(orig_mean) and np.isfinite(new_mean):\n",
    "                    imputation_type = \"Mean\" if abs(orig_mean - new_mean) < 1e-3 else \"Median\"\n",
    "                else:\n",
    "                    imputation_type = \"Unknown\"\n",
    "                imputation_report[col] = f\"Numerical → {imputation_type} Imputation\"\n",
    "\n",
    "    # ✅ Handle categorical columns\n",
    "    for col in categorical_cols:\n",
    "        if col in original_df.columns and col in df.columns:\n",
    "            if original_df[col].isnull().sum() > 0 and df[col].isnull().sum() == 0:\n",
    "                imputation_report[col] = \"Categorical → Mode Imputation\"\n",
    "\n",
    "    # ✅ Handle datetime columns (only if provided)\n",
    "    if datetime_cols:\n",
    "        for col in datetime_cols:\n",
    "            if col in original_df.columns and col in df.columns:\n",
    "                if original_df[col].isnull().sum() > 0 and df[col].isnull().sum() == 0:\n",
    "                    imputation_report[col] = \"Datetime → Forward/Backward Fill\"\n",
    "\n",
    "    # ✅ Final Report\n",
    "    print(\"\\n🧩 Imputation Report:\")\n",
    "    if imputation_report:\n",
    "        for col, method in imputation_report.items():\n",
    "            print(f\" - {col}: {method}\")\n",
    "    else:\n",
    "        print(\"No imputations were detected.\")\n",
    "\n",
    "    return imputation_report\n",
    "\n",
    "\n",
    "generate_imputation_report(original_df, df, numerical_cols, categorical_cols)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d549ef25-77f1-4cbd-a0bf-ec319ecb74d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧭 Choose Outlier Detection Method:\n",
      "Options: iqr | zscore | isolation\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter method (default='iqr'):  iqr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🛠️ Choose Outlier Treatment Action:\n",
      "Options: remove | cap | transform | none\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter action (default='cap'):  cap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Step 10: Outlier Detection using 'iqr' and Treatment: 'cap'\n",
      "\n",
      "📋 Outlier Treatment Report:\n",
      " - Age: 66 outliers → Outliers capped at IQR bounds\n",
      " - Parch: 213 outliers → Outliers capped at IQR bounds\n",
      " - Fare: 116 outliers → Outliers capped at IQR bounds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked_missing</th>\n",
       "      <th>Age_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.6344</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>female</td>\n",
       "      <td>113803</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male</td>\n",
       "      <td>373450</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sex            Ticket Embarked  Pclass   Age  Parch     Fare  \\\n",
       "0    male         A/5 21171        S       3  22.0    0.0   7.2500   \n",
       "1  female          PC 17599        C       1  38.0    0.0  65.6344   \n",
       "2  female  STON/O2. 3101282        S       3  26.0    0.0   7.9250   \n",
       "3  female            113803        S       1  35.0    0.0  53.1000   \n",
       "4    male            373450        S       3  35.0    0.0   8.0500   \n",
       "\n",
       "   Embarked_missing  Age_missing  \n",
       "0                 0            0  \n",
       "1                 0            0  \n",
       "2                 0            0  \n",
       "3                 0            0  \n",
       "4                 0            0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from scipy.stats import zscore\n",
    "\n",
    "def step10_detect_and_treat_outliers(df, numerical_cols, method='iqr', action='cap', z_thresh=3.0, contamination=0.01):\n",
    "    \"\"\"\n",
    "    Detects and treats outliers using IQR, Z-Score, or Isolation Forest.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "        numerical_cols (list): List of numerical columns to process\n",
    "        method (str): Outlier detection method: 'iqr', 'zscore', 'isolation'\n",
    "        action (str): Action on outliers: 'remove', 'cap', 'log', or 'none'\n",
    "        z_thresh (float): Threshold for z-score method\n",
    "        contamination (float): Expected % of outliers for IsolationForest\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame after outlier treatment\n",
    "        dict: Outlier report per column\n",
    "    \"\"\"\n",
    "    outlier_report = {}\n",
    "\n",
    "    print(f\"\\n🔍 Step 10: Outlier Detection using '{method}' and Treatment: '{action}'\")\n",
    "\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    for col in numerical_cols:\n",
    "        original_values = df_processed[col].copy()\n",
    "        col_outliers = None\n",
    "\n",
    "        if method == 'iqr':\n",
    "            # ➤ IQR Method\n",
    "            Q1 = df_processed[col].quantile(0.25)\n",
    "            Q3 = df_processed[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            col_outliers = (df_processed[col] < lower_bound) | (df_processed[col] > upper_bound)\n",
    "\n",
    "        elif method == 'zscore':\n",
    "            # ➤ Z-Score Method\n",
    "            z_scores = zscore(df_processed[col].dropna())\n",
    "            outlier_idx = abs(z_scores) > z_thresh\n",
    "            col_outliers = pd.Series(False, index=df_processed.index)\n",
    "            col_outliers[df_processed[col].dropna().index[outlier_idx]] = True\n",
    "\n",
    "        elif method == 'isolation':\n",
    "            # ➤ Isolation Forest Method\n",
    "            model = IsolationForest(contamination=contamination, random_state=42)\n",
    "            preds = model.fit_predict(df_processed[[col]].dropna())\n",
    "            col_outliers = pd.Series(False, index=df_processed.index)\n",
    "            col_outliers[df_processed[col].dropna().index] = (preds == -1)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid method. Choose from 'iqr', 'zscore', or 'isolation'.\")\n",
    "\n",
    "        outlier_count = col_outliers.sum()\n",
    "        treatment = None\n",
    "\n",
    "        # Treatment\n",
    "        if outlier_count > 0:\n",
    "            if action == 'remove':\n",
    "                df_processed = df_processed[~col_outliers]\n",
    "                treatment = f\"{outlier_count} outliers removed\"\n",
    "            elif action == 'cap':\n",
    "                if method == 'iqr':\n",
    "                    df_processed[col] = np.where(df_processed[col] > upper_bound, upper_bound,\n",
    "                                                 np.where(df_processed[col] < lower_bound, lower_bound, df_processed[col]))\n",
    "                    treatment = f\"Outliers capped at IQR bounds\"\n",
    "                elif method == 'zscore':\n",
    "                    mean = df_processed[col].mean()\n",
    "                    std = df_processed[col].std()\n",
    "                    upper = mean + z_thresh * std\n",
    "                    lower = mean - z_thresh * std\n",
    "                    df_processed[col] = np.clip(df_processed[col], lower, upper)\n",
    "                    treatment = f\"Outliers capped at z-score ±{z_thresh}\"\n",
    "            elif action == 'log':\n",
    "                df_processed[col] = np.log1p(df_processed[col].clip(lower=0))\n",
    "                treatment = f\"Log transformation applied\"\n",
    "            elif action == 'none':\n",
    "                treatment = f\"Outliers detected but not treated\"\n",
    "\n",
    "            outlier_report[col] = {\n",
    "                \"method\": method,\n",
    "                \"outliers_detected\": int(outlier_count),\n",
    "                \"treatment\": treatment\n",
    "            }\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n📋 Outlier Treatment Report:\")\n",
    "    if outlier_report:\n",
    "        for col, details in outlier_report.items():\n",
    "            print(f\" - {col}: {details['outliers_detected']} outliers → {details['treatment']}\")\n",
    "    else:\n",
    "        print(\"✅ No significant outliers found or treated.\")\n",
    "\n",
    "    return df_processed, outlier_report\n",
    "\n",
    "# Ensure valid numerical columns\n",
    "numerical_cols = [col for col in numerical_cols if col in df.columns]\n",
    "\n",
    "\n",
    "# ---------------------- User Input for Outlier Handling ------------------------\n",
    "print(\"\\n🧭 Choose Outlier Detection Method:\")\n",
    "print(\"Options: iqr | zscore | isolation\")\n",
    "method = input(\"Enter method (default='iqr'): \").strip().lower()\n",
    "if method not in ['iqr', 'zscore', 'isolation']:\n",
    "    print(\"⚠️ Invalid method. Defaulting to 'iqr'\")\n",
    "    method = 'iqr'\n",
    "\n",
    "print(\"\\n🛠️ Choose Outlier Treatment Action:\")\n",
    "print(\"Options: remove | cap | transform | none\")\n",
    "action = input(\"Enter action (default='cap'): \").strip().lower()\n",
    "if action not in ['remove', 'cap', 'transform', 'none']:\n",
    "    print(\"⚠️ Invalid action. Defaulting to 'cap'\")\n",
    "    action = 'cap'\n",
    "\n",
    "# Optional: Ask Z-score threshold or Isolation Forest contamination level if needed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Call the outlier detection function\n",
    "df, outlier_summary = step10_detect_and_treat_outliers(\n",
    "    df=df,\n",
    "    numerical_cols=numerical_cols,\n",
    "    method=method,\n",
    "    action=action\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae5294d7-7bc8-4fe3-834c-0ad1eff77fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔧 Step 11: Data Type Optimization and Standardization\n",
      "\n",
      "➤ Optimizing numeric types for memory efficiency...\n",
      "✅ Numeric type downcasting complete.\n",
      "\n",
      "➤ Standardizing string-based categorical columns...\n",
      "✅ String standardization (case, spacing) complete.\n",
      "\n",
      "➤ Converting columns to datetime where applicable...\n",
      "\n",
      "🚨 Checking for impossible values in numeric columns...\n",
      "✅ No invalid numerical ranges detected.\n"
     ]
    }
   ],
   "source": [
    "def step11_optimize_and_standardize(df):\n",
    "    \"\"\"\n",
    "    Step 11: Optimizes data types for memory efficiency and standardizes formatting across the dataset.\n",
    "\n",
    "    Tasks performed:\n",
    "    - Convert numerical columns to optimal types\n",
    "    - Standardize categorical strings (case, whitespace)\n",
    "    - Ensure datetime formatting consistency\n",
    "    - Validate numeric ranges and flag anomalies\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Optimized and standardized DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n🔧 Step 11: Data Type Optimization and Standardization\")\n",
    "\n",
    "    # 🔢 1. Convert numeric columns to smaller dtypes\n",
    "    print(\"\\n➤ Optimizing numeric types for memory efficiency...\")\n",
    "    for col in df.select_dtypes(include=['int64', 'float64','float32']).columns:\n",
    "        if pd.api.types.is_integer_dtype(df[col]):\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "        elif pd.api.types.is_float_dtype(df[col]):\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    print(\"✅ Numeric type downcasting complete.\")\n",
    "\n",
    "    # 🔤 2. Standardize categorical string columns\n",
    "    print(\"\\n➤ Standardizing string-based categorical columns...\")\n",
    "    str_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in str_cols:\n",
    "        df[col] = df[col].astype(str).str.strip().str.lower().str.replace(r'\\s+', ' ', regex=True)\n",
    "    print(\"✅ String standardization (case, spacing) complete.\")\n",
    "\n",
    "    # 🕒 3. Ensure consistent datetime formatting\n",
    "    print(\"\\n➤ Converting columns to datetime where applicable...\")\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            sample = df[col].dropna().astype(str).head(10)\n",
    "            if sample.str.match(r'\\d{4}-\\d{2}-\\d{2}').any():  # basic YYYY-MM-DD check\n",
    "                try:\n",
    "                    df[col] = pd.to_datetime(df[col], errors='coerce', utc=True)\n",
    "                    print(f\"🕓 Converted '{col}' to datetime with UTC format.\")\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "    # 🔍 4. Identify invalid numeric values (e.g., negative ages, 0 prices)\n",
    "    print(\"\\n🚨 Checking for impossible values in numeric columns...\")\n",
    "    flagged = []\n",
    "    for col in df.select_dtypes(include=['int', 'float']).columns:\n",
    "        if df[col].min() < 0:\n",
    "            flagged.append(col)\n",
    "    if flagged:\n",
    "        print(f\"⚠️ Columns with potential invalid negative values: {flagged}\")\n",
    "    else:\n",
    "        print(\"✅ No invalid numerical ranges detected.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "df = step11_optimize_and_standardize(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ae42afb3-44c8-444e-9a89-b9296c613287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked_missing</th>\n",
       "      <th>Age_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>a/5 21171</td>\n",
       "      <td>s</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>pc 17599</td>\n",
       "      <td>c</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.634399</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female</td>\n",
       "      <td>ston/o2. 3101282</td>\n",
       "      <td>s</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.925000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>female</td>\n",
       "      <td>113803</td>\n",
       "      <td>s</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.099998</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male</td>\n",
       "      <td>373450</td>\n",
       "      <td>s</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.050000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sex            Ticket Embarked  Pclass   Age  Parch       Fare  \\\n",
       "0    male         a/5 21171        s       3  22.0    0.0   7.250000   \n",
       "1  female          pc 17599        c       1  38.0    0.0  65.634399   \n",
       "2  female  ston/o2. 3101282        s       3  26.0    0.0   7.925000   \n",
       "3  female            113803        s       1  35.0    0.0  53.099998   \n",
       "4    male            373450        s       3  35.0    0.0   8.050000   \n",
       "\n",
       "   Embarked_missing  Age_missing  \n",
       "0                 0            0  \n",
       "1                 0            0  \n",
       "2                 0            0  \n",
       "3                 0            0  \n",
       "4                 0            0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e7ec3c-5bda-4007-adcc-085c11c29e2c",
   "metadata": {},
   "source": [
    "Phase 2.2: Feature Transformation and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5d597013-56c4-4211-bde8-828f386696cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 Choose a scaling method:\n",
      " - 'auto'      : Automatically chooses based on skewness\n",
      " - 'standard'  : StandardScaler for normally distributed data\n",
      " - 'minmax'    : MinMaxScaler for bounded range [0,1]\n",
      " - 'robust'    : RobustScaler for data with outliers\n",
      " - 'quantile'  : QuantileTransformer (uniform distribution)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your scaling method:  auto\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Step 12: Scaling Numerical Features\n",
      "✅ Scaled Pclass using RobustScaler (Moderate outliers)\n",
      "✅ Scaled Age using StandardScaler (Normal dist)\n",
      "⚠️ Skipping Parch (constant or quasi-constant)\n",
      "✅ Scaled Fare using RobustScaler (Moderate outliers)\n",
      "✅ Scaled Embarked_missing using QuantileTransformer (Highly skewed)\n",
      "✅ Scaled Age_missing using RobustScaler (Moderate outliers)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, QuantileTransformer\n",
    "from scipy.stats import skew\n",
    "\n",
    "# Ask user for scaling strategy\n",
    "valid_strategies = ['auto', 'standard', 'minmax', 'robust', 'quantile']\n",
    "print(\"\\n📌 Choose a scaling method:\")\n",
    "print(\" - 'auto'      : Automatically chooses based on skewness\")\n",
    "print(\" - 'standard'  : StandardScaler for normally distributed data\")\n",
    "print(\" - 'minmax'    : MinMaxScaler for bounded range [0,1]\")\n",
    "print(\" - 'robust'    : RobustScaler for data with outliers\")\n",
    "print(\" - 'quantile'  : QuantileTransformer (uniform distribution)\")\n",
    "\n",
    "user_strategy = input(\"\\nEnter your scaling method: \").strip().lower()\n",
    "if user_strategy not in valid_strategies:\n",
    "    print(f\"⚠️ Invalid input. Defaulting to 'auto'\")\n",
    "    user_strategy = 'auto'\n",
    "\n",
    "\n",
    "def step12_scale_numerical_features(df, numerical_cols, strategy='auto'):\n",
    "    \"\"\"\n",
    "    Scales numerical features using a chosen strategy.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "        numerical_cols (list): Numerical feature names\n",
    "        strategy (str): Chosen method from ['auto', 'standard', 'minmax', 'robust', 'quantile']\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Scaled dataframe\n",
    "        dict: Dictionary showing scaler used per column\n",
    "    \"\"\"\n",
    "    print(\"\\n🔄 Step 12: Scaling Numerical Features\")\n",
    "    df_scaled = df.copy()\n",
    "    scaler_used = {}\n",
    "\n",
    "    for col in numerical_cols:\n",
    "        if df_scaled[col].nunique() <= 1:\n",
    "            print(f\"⚠️ Skipping {col} (constant or quasi-constant)\")\n",
    "            continue\n",
    "\n",
    "        data = df_scaled[col].values.reshape(-1, 1)\n",
    "        scaler = None\n",
    "\n",
    "        if strategy == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "            strategy_used = \"StandardScaler\"\n",
    "        elif strategy == 'minmax':\n",
    "            scaler = MinMaxScaler()\n",
    "            strategy_used = \"MinMaxScaler\"\n",
    "        elif strategy == 'robust':\n",
    "            scaler = RobustScaler()\n",
    "            strategy_used = \"RobustScaler\"\n",
    "        elif strategy == 'quantile':\n",
    "            scaler = QuantileTransformer(output_distribution='uniform')\n",
    "            strategy_used = \"QuantileTransformer (Uniform)\"\n",
    "        elif strategy == 'auto':\n",
    "            col_skew = skew(df_scaled[col].dropna())\n",
    "            if abs(col_skew) < 0.5:\n",
    "                scaler = StandardScaler()\n",
    "                strategy_used = \"StandardScaler (Normal dist)\"\n",
    "            elif abs(col_skew) < 2:\n",
    "                scaler = RobustScaler()\n",
    "                strategy_used = \"RobustScaler (Moderate outliers)\"\n",
    "            else:\n",
    "                scaler = QuantileTransformer(output_distribution='uniform')\n",
    "                strategy_used = \"QuantileTransformer (Highly skewed)\"\n",
    "        else:\n",
    "            raise ValueError(\"Invalid strategy\")\n",
    "\n",
    "        try:\n",
    "            df_scaled[col] = scaler.fit_transform(data)\n",
    "            scaler_used[col] = strategy_used\n",
    "            print(f\"✅ Scaled {col} using {strategy_used}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Could not scale {col}: {e}\")\n",
    "\n",
    "    return df_scaled, scaler_used\n",
    "\n",
    "\n",
    "# Run the function with user's selected strategy\n",
    "numerical_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "df_scaled, scaling_report = step12_scale_numerical_features(df, numerical_cols, strategy=user_strategy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ef85a662-b0a6-4721-9328-11a8d845f378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 Choose a transformation strategy:\n",
      " - 'auto'        : Let system decide per column (based on skew)\n",
      " - 'log'         : Log transform (for positive right-skewed)\n",
      " - 'sqrt'        : Square-root transform (for counts)\n",
      " - 'boxcox'      : Box-Cox (positive values only)\n",
      " - 'yeojohnson'  : Yeo-Johnson (supports zero/negative)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your transformation strategy:  auto\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pclass: none (skew ~ normal)\n",
      "✅ Age: none (skew ~ normal)\n",
      "⚠️ Skipping Parch (constant/quasi-constant)\n",
      "✅ Fare: sqrt\n",
      "✅ Embarked_missing: sqrt\n",
      "✅ Age_missing: sqrt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked_missing</th>\n",
       "      <th>Age_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>a/5 21171</td>\n",
       "      <td>s</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.692582</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>pc 17599</td>\n",
       "      <td>c</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.101506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female</td>\n",
       "      <td>ston/o2. 3101282</td>\n",
       "      <td>s</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.815138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>female</td>\n",
       "      <td>113803</td>\n",
       "      <td>s</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.286974</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male</td>\n",
       "      <td>373450</td>\n",
       "      <td>s</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.837252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sex            Ticket Embarked  Pclass   Age  Parch      Fare  \\\n",
       "0    male         a/5 21171        s       3  22.0    0.0  2.692582   \n",
       "1  female          pc 17599        c       1  38.0    0.0  8.101506   \n",
       "2  female  ston/o2. 3101282        s       3  26.0    0.0  2.815138   \n",
       "3  female            113803        s       1  35.0    0.0  7.286974   \n",
       "4    male            373450        s       3  35.0    0.0  2.837252   \n",
       "\n",
       "   Embarked_missing  Age_missing  \n",
       "0               0.0          0.0  \n",
       "1               0.0          0.0  \n",
       "2               0.0          0.0  \n",
       "3               0.0          0.0  \n",
       "4               0.0          0.0  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import boxcox, yeojohnson, skew\n",
    "\n",
    "\n",
    "# Ask user for strategy\n",
    "print(\"\\n📌 Choose a transformation strategy:\")\n",
    "print(\" - 'auto'        : Let system decide per column (based on skew)\")\n",
    "print(\" - 'log'         : Log transform (for positive right-skewed)\")\n",
    "print(\" - 'sqrt'        : Square-root transform (for counts)\")\n",
    "print(\" - 'boxcox'      : Box-Cox (positive values only)\")\n",
    "print(\" - 'yeojohnson'  : Yeo-Johnson (supports zero/negative)\")\n",
    "\n",
    "user_transform = input(\"\\nEnter your transformation strategy: \").strip().lower()\n",
    "valid_transforms = ['auto', 'log', 'sqrt', 'boxcox', 'yeojohnson']\n",
    "\n",
    "if user_transform not in valid_transforms:\n",
    "    print(f\"⚠️ Invalid input. Defaulting to 'auto'\")\n",
    "    user_transform = 'auto'\n",
    "\n",
    "\n",
    "def step13_transform_distributions(df, numerical_cols, strategy='auto'):\n",
    "    \"\"\"\n",
    "    #Apply transformations to achieve normality:\n",
    "        ->Log transformation for right-skewed data\n",
    "        ->Square root transformation for count data\n",
    "        ->Box-Cox transformation for general normalization\n",
    "        ->Yeo-Johnson for data including zero/negative values\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Dataframe containing numerical features\n",
    "        numerical_cols (list): List of numerical column names\n",
    "        strategy (str): One of ['auto', 'log', 'sqrt', 'boxcox', 'yeojohnson']\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Transformed dataframe\n",
    "        dict: Dictionary of transformation strategy used per column\n",
    "    \"\"\"\n",
    "    df_transformed = df.copy()\n",
    "    transform_used = {}\n",
    "\n",
    "    for col in numerical_cols:\n",
    "        if df_transformed[col].nunique() <= 1:\n",
    "            print(f\"⚠️ Skipping {col} (constant/quasi-constant)\")\n",
    "            continue\n",
    "\n",
    "        col_skew = skew(df_transformed[col].dropna())\n",
    "        data = df_transformed[col]\n",
    "\n",
    "        try:\n",
    "            if strategy == 'log' or (strategy == 'auto' and col_skew > 1 and (data > 0).all()):\n",
    "                df_transformed[col] = np.log1p(data)\n",
    "                transform_used[col] = 'log1p'\n",
    "\n",
    "            elif strategy == 'sqrt' or (strategy == 'auto' and (data >= 0).all() and col_skew > 0.5):\n",
    "                df_transformed[col] = np.sqrt(data)\n",
    "                transform_used[col] = 'sqrt'\n",
    "\n",
    "            elif strategy == 'boxcox' or (strategy == 'auto' and col_skew > 0.5 and (data > 0).all()):\n",
    "                df_transformed[col], _ = boxcox(data + 1e-4)  # tiny shift if near-zero\n",
    "                transform_used[col] = 'boxcox'\n",
    "\n",
    "            elif strategy == 'yeojohnson' or (strategy == 'auto' and not (data > 0).all()):\n",
    "                df_transformed[col], _ = yeojohnson(data)\n",
    "                transform_used[col] = 'yeojohnson'\n",
    "\n",
    "            elif strategy == 'auto':\n",
    "                transform_used[col] = 'none (skew ~ normal)'\n",
    "\n",
    "            print(f\"✅ {col}: {transform_used[col]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Skipping {col} due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "    return df_transformed, transform_used\n",
    "\n",
    "\n",
    "# Run the function\n",
    "df, transformation_report = step13_transform_distributions(df, numerical_cols, strategy=user_transform)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e39ea65e-8535-4b88-b169-4187a8c6e878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📦 Choose encoding strategy (auto, onehot, target, ordinal, binary, hash):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter strategy (default = auto):  auto\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧾 Encoding Summary:\n",
      "❌ Skipping Name (not in dataframe)\n",
      "\n",
      "➡️ Encoding Column: 'Sex' | Unique Values: 2\n",
      "✅ Encoded 'Sex' using strategy: onehot\n",
      "\n",
      "➡️ Encoding Column: 'Ticket' | Unique Values: 681\n",
      "❌ Failed to encode 'Ticket': name 'HashingEncoder' is not defined\n",
      "\n",
      "➡️ Encoding Column: 'Embarked' | Unique Values: 3\n",
      "✅ Encoded 'Embarked' using strategy: onehot\n"
     ]
    }
   ],
   "source": [
    "def step14_encode_categorical_features(df, y, categorical_cols, strategy='auto'):\n",
    "    \"\"\"\n",
    "    Encodes categorical features based on strategy and cardinality.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing features.\n",
    "        y (pd.Series): Target variable.\n",
    "        categorical_cols (list): List of categorical column names.\n",
    "        strategy (str): Encoding strategy.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Transformed dataframe\n",
    "        dict: Encoding method used per column\n",
    "    \"\"\"\n",
    "    df_encoded = df.copy()\n",
    "    encoding_used = {}\n",
    "\n",
    "    print(\"\\n🧾 Encoding Summary:\")\n",
    "    for col in categorical_cols:\n",
    "        if col not in df_encoded.columns:\n",
    "            print(f\"❌ Skipping {col} (not in dataframe)\")\n",
    "            continue\n",
    "\n",
    "        n_unique = df_encoded[col].nunique()\n",
    "        print(f\"\\n➡️ Encoding Column: '{col}' | Unique Values: {n_unique}\")\n",
    "\n",
    "        try:\n",
    "            if strategy == 'auto':\n",
    "                if n_unique <= 10:\n",
    "                    df_encoded = pd.get_dummies(df_encoded, columns=[col], drop_first=True)\n",
    "                    encoding_used[col] = 'onehot'\n",
    "                elif 10 < n_unique <= 50:\n",
    "                    encoder = BinaryEncoder(cols=[col], drop_invariant=True)\n",
    "                    df_encoded = encoder.fit_transform(df_encoded)\n",
    "                    encoding_used[col] = 'binary'\n",
    "                elif n_unique > 50:\n",
    "                    encoder = HashingEncoder(cols=[col], n_components=8)\n",
    "                    df_encoded = encoder.fit_transform(df_encoded)\n",
    "                    encoding_used[col] = 'hash'\n",
    "            elif strategy == 'onehot':\n",
    "                df_encoded = pd.get_dummies(df_encoded, columns=[col], drop_first=True)\n",
    "                encoding_used[col] = 'onehot'\n",
    "            elif strategy == 'target':\n",
    "                encoder = TargetEncoder(cols=[col])\n",
    "                df_encoded[col] = encoder.fit_transform(df_encoded[col], y)\n",
    "                encoding_used[col] = 'target'\n",
    "            elif strategy == 'ordinal':\n",
    "                encoder = OrdinalEncoder(cols=[col])\n",
    "                df_encoded[col] = encoder.fit_transform(df_encoded[col])\n",
    "                encoding_used[col] = 'ordinal'\n",
    "            elif strategy == 'binary':\n",
    "                encoder = BinaryEncoder(cols=[col])\n",
    "                df_encoded = encoder.fit_transform(df_encoded)\n",
    "                encoding_used[col] = 'binary'\n",
    "            elif strategy == 'hash':\n",
    "                encoder = HashingEncoder(cols=[col], n_components=8)\n",
    "                df_encoded = encoder.fit_transform(df_encoded)\n",
    "                encoding_used[col] = 'hash'\n",
    "\n",
    "            print(f\"✅ Encoded '{col}' using strategy: {encoding_used[col]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to encode '{col}': {e}\")\n",
    "            continue\n",
    "\n",
    "    return df_encoded, encoding_used\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n📦 Choose encoding strategy (auto, onehot, target, ordinal, binary, hash):\")\n",
    "encoding_strategy = input(\"Enter strategy (default = auto): \").strip().lower()\n",
    "if encoding_strategy not in ['auto', 'onehot', 'target', 'ordinal', 'binary', 'hash']:\n",
    "    encoding_strategy = 'auto'\n",
    "\n",
    "# Run the encoding function\n",
    "df, encoding_report = step14_encode_categorical_features(df, y, categorical_cols, strategy=encoding_strategy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e6e1687c-9f4e-45f1-9ebd-83dc0cfbd462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked_missing</th>\n",
       "      <th>Age_missing</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_q</th>\n",
       "      <th>Embarked_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a/5 21171</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.692582</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pc 17599</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.101506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ston/o2. 3101282</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.815138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>113803</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.286974</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>373450</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.837252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Ticket  Pclass   Age  Parch      Fare  Embarked_missing  \\\n",
       "0         a/5 21171       3  22.0    0.0  2.692582               0.0   \n",
       "1          pc 17599       1  38.0    0.0  8.101506               0.0   \n",
       "2  ston/o2. 3101282       3  26.0    0.0  2.815138               0.0   \n",
       "3            113803       1  35.0    0.0  7.286974               0.0   \n",
       "4            373450       3  35.0    0.0  2.837252               0.0   \n",
       "\n",
       "   Age_missing  Sex_male  Embarked_q  Embarked_s  \n",
       "0          0.0      True       False        True  \n",
       "1          0.0     False       False       False  \n",
       "2          0.0     False       False        True  \n",
       "3          0.0     False       False        True  \n",
       "4          0.0      True       False        True  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa6d342-be85-45fe-9b28-bae7810d472d",
   "metadata": {},
   "source": [
    "Phase 2.3: Feature Creation and Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7223e21d-52a4-47f7-bbaa-557e62abfac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 Step 15: Domain-Specific Feature Engineering\n",
      "✅ Added 2 domain-specific engineered features:\n",
      " - AgeFareInteraction\n",
      " - Ticket_char_count\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AgeFareInteraction</th>\n",
       "      <th>Ticket_char_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59.236813</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>307.857239</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73.193581</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>255.044098</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>99.303825</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AgeFareInteraction  Ticket_char_count\n",
       "0           59.236813                  9\n",
       "1          307.857239                  8\n",
       "2           73.193581                 16\n",
       "3          255.044098                  6\n",
       "4           99.303825                  6"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def step15_domain_specific_feature_engineering(df):\n",
    "    \"\"\"\n",
    "    Performs domain-specific feature engineering and returns a separate DataFrame\n",
    "    containing only the new features.\n",
    "    \"\"\"\n",
    "    from geopy.distance import geodesic\n",
    "    import pandas as pd\n",
    "\n",
    "    print(\"\\n🧠 Step 15: Domain-Specific Feature Engineering\")\n",
    "\n",
    "    engineered_data = {}\n",
    "    new_features = []\n",
    "\n",
    "    # 1. Safe combinations\n",
    "    if all(col in df.columns for col in ['SibSp', 'Parch']):\n",
    "        engineered_data['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "        new_features.append(\"FamilySize\")\n",
    "\n",
    "    if 'Age' in df.columns and 'Fare' in df.columns:\n",
    "        engineered_data['AgeFareInteraction'] = df['Age'] * df['Fare']\n",
    "        new_features.append(\"AgeFareInteraction\")\n",
    "\n",
    "    if 'Income' in df.columns and 'Age' in df.columns:\n",
    "        engineered_data['IncomeToAge'] = df['Income'] / (df['Age'] + 1e-3)\n",
    "        new_features.append(\"IncomeToAge\")\n",
    "\n",
    "    if 'Weight' in df.columns and 'Height' in df.columns:\n",
    "        engineered_data['BMI'] = df['Weight'] / ((df['Height'] / 100) ** 2)\n",
    "        new_features.append(\"BMI\")\n",
    "\n",
    "    # 2. Datetime features\n",
    "    datetime_cols = df.select_dtypes(include=['datetime64[ns]']).columns\n",
    "    for col in datetime_cols:\n",
    "        engineered_data[f'{col}_month'] = df[col].dt.month\n",
    "        engineered_data[f'{col}_weekday'] = df[col].dt.weekday\n",
    "        engineered_data[f'{col}_hour'] = df[col].dt.hour\n",
    "        new_features.extend([f'{col}_month', f'{col}_weekday', f'{col}_hour'])\n",
    "\n",
    "    # 3. Geospatial\n",
    "    if all(col in df.columns for col in ['latitude', 'longitude']):\n",
    "        center = (df['latitude'].mean(), df['longitude'].mean())\n",
    "        engineered_data['Distance_to_Center'] = df.apply(\n",
    "            lambda row: geodesic(center, (row['latitude'], row['longitude'])).km, axis=1\n",
    "        )\n",
    "        new_features.append(\"Distance_to_Center\")\n",
    "\n",
    "    # 4. Text features\n",
    "    text_cols = df.select_dtypes(include='object').columns\n",
    "    for col in text_cols:\n",
    "        if df[col].apply(lambda x: isinstance(x, str)).mean() > 0.9:\n",
    "            engineered_data[f'{col}_char_count'] = df[col].apply(lambda x: len(str(x)))\n",
    "            new_features.append(f'{col}_char_count')\n",
    "\n",
    "    # Create new DataFrame with only engineered features\n",
    "    new_features_df = pd.DataFrame(engineered_data)\n",
    "\n",
    "    print(f\"✅ Added {len(new_features)} domain-specific engineered features:\")\n",
    "    for feat in new_features:\n",
    "        print(f\" - {feat}\")\n",
    "\n",
    "    return new_features_df\n",
    "\n",
    "engineered_features_df = step15_domain_specific_feature_engineering(df)\n",
    "\n",
    "#original df stays intact, and engineered_features_df contains only the new columns.\n",
    "\n",
    "engineered_features_df.head()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "77e14842-b0ef-4cb9-ac4c-b7cf73fa2094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked_missing</th>\n",
       "      <th>Age_missing</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_q</th>\n",
       "      <th>Embarked_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a/5 21171</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.692582</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pc 17599</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.101506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ston/o2. 3101282</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.815138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>113803</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.286974</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>373450</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.837252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Ticket  Pclass   Age  Parch      Fare  Embarked_missing  \\\n",
       "0         a/5 21171       3  22.0    0.0  2.692582               0.0   \n",
       "1          pc 17599       1  38.0    0.0  8.101506               0.0   \n",
       "2  ston/o2. 3101282       3  26.0    0.0  2.815138               0.0   \n",
       "3            113803       1  35.0    0.0  7.286974               0.0   \n",
       "4            373450       3  35.0    0.0  2.837252               0.0   \n",
       "\n",
       "   Age_missing  Sex_male  Embarked_q  Embarked_s  \n",
       "0          0.0      True       False        True  \n",
       "1          0.0     False       False       False  \n",
       "2          0.0     False       False        True  \n",
       "3          0.0     False       False        True  \n",
       "4          0.0      True       False        True  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "53efcdcf-7d16-4649-860d-171481958010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Creating polynomial & interaction features...\n",
      "✅ Added 15 interaction/polynomial features.\n",
      "\n",
      "🔄 Generating group-wise aggregate features...\n",
      "✅ Aggregates created for categorical-numerical combinations.\n",
      "\n",
      "🧠 Mathematical feature engineering complete.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass Age</th>\n",
       "      <th>Pclass Parch</th>\n",
       "      <th>Pclass Fare</th>\n",
       "      <th>Pclass Embarked_missing</th>\n",
       "      <th>Pclass Age_missing</th>\n",
       "      <th>Age Parch</th>\n",
       "      <th>Age Fare</th>\n",
       "      <th>Age Embarked_missing</th>\n",
       "      <th>Age Age_missing</th>\n",
       "      <th>Parch Fare</th>\n",
       "      <th>...</th>\n",
       "      <th>Ticket_Age_mean</th>\n",
       "      <th>Ticket_Age_std</th>\n",
       "      <th>Ticket_Parch_mean</th>\n",
       "      <th>Ticket_Parch_std</th>\n",
       "      <th>Ticket_Fare_mean</th>\n",
       "      <th>Ticket_Fare_std</th>\n",
       "      <th>Ticket_Embarked_missing_mean</th>\n",
       "      <th>Ticket_Embarked_missing_std</th>\n",
       "      <th>Ticket_Age_missing_mean</th>\n",
       "      <th>Ticket_Age_missing_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.077747</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.236813</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.692582</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.101506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>307.857239</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.101506</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.445413</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.193581</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.815138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.286974</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>255.044098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.286974</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.511757</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.303825</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.837252</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass Age  Pclass Parch  Pclass Fare  Pclass Embarked_missing  \\\n",
       "0        66.0           0.0     8.077747                      0.0   \n",
       "1        38.0           0.0     8.101506                      0.0   \n",
       "2        78.0           0.0     8.445413                      0.0   \n",
       "3        35.0           0.0     7.286974                      0.0   \n",
       "4       105.0           0.0     8.511757                      0.0   \n",
       "\n",
       "   Pclass Age_missing  Age Parch    Age Fare  Age Embarked_missing  \\\n",
       "0                 0.0        0.0   59.236813                   0.0   \n",
       "1                 0.0        0.0  307.857239                   0.0   \n",
       "2                 0.0        0.0   73.193581                   0.0   \n",
       "3                 0.0        0.0  255.044098                   0.0   \n",
       "4                 0.0        0.0   99.303825                   0.0   \n",
       "\n",
       "   Age Age_missing  Parch Fare  ...  Ticket_Age_mean  Ticket_Age_std  \\\n",
       "0              0.0         0.0  ...             22.0             NaN   \n",
       "1              0.0         0.0  ...             38.0             NaN   \n",
       "2              0.0         0.0  ...             26.0             NaN   \n",
       "3              0.0         0.0  ...             36.0        1.414214   \n",
       "4              0.0         0.0  ...             35.0             NaN   \n",
       "\n",
       "   Ticket_Parch_mean  Ticket_Parch_std  Ticket_Fare_mean  Ticket_Fare_std  \\\n",
       "0                0.0               NaN          2.692582              NaN   \n",
       "1                0.0               NaN          8.101506              NaN   \n",
       "2                0.0               NaN          2.815138              NaN   \n",
       "3                0.0               0.0          7.286974              0.0   \n",
       "4                0.0               NaN          2.837252              NaN   \n",
       "\n",
       "   Ticket_Embarked_missing_mean  Ticket_Embarked_missing_std  \\\n",
       "0                           0.0                          NaN   \n",
       "1                           0.0                          NaN   \n",
       "2                           0.0                          NaN   \n",
       "3                           0.0                          0.0   \n",
       "4                           0.0                          NaN   \n",
       "\n",
       "   Ticket_Age_missing_mean  Ticket_Age_missing_std  \n",
       "0                      0.0                     NaN  \n",
       "1                      0.0                     NaN  \n",
       "2                      0.0                     NaN  \n",
       "3                      0.0                     0.0  \n",
       "4                      0.0                     NaN  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def step16_mathematical_feature_engineering(df, datetime_col=None):\n",
    "    \"\"\"\n",
    "    Performs mathematical feature engineering and returns a new DataFrame\n",
    "    with only the newly created features (original df remains unchanged).\n",
    "\n",
    "    Includes:\n",
    "    - Polynomial and interaction terms\n",
    "    - Moving average and lag features (if datetime is given)\n",
    "    - Group-wise aggregates (if categorical columns present)\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Original dataset\n",
    "        datetime_col (str): Name of datetime column for time-based features\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: New DataFrame with only engineered features\n",
    "    \"\"\"\n",
    "    df_temp = df.copy()\n",
    "    new_features_df = pd.DataFrame(index=df_temp.index)\n",
    "\n",
    "    # --- Identify numeric and categorical columns ---\n",
    "    numerical_cols = df_temp.select_dtypes(include='number').columns.tolist()\n",
    "    cat_cols = df_temp.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    # --- Polynomial and Interaction Features ---\n",
    "    if len(numerical_cols) >= 2:\n",
    "        print(\"\\n📈 Creating polynomial & interaction features...\")\n",
    "        poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "        poly_features = poly.fit_transform(df_temp[numerical_cols].fillna(0))\n",
    "        poly_feature_names = poly.get_feature_names_out(numerical_cols)\n",
    "\n",
    "        poly_df = pd.DataFrame(poly_features, columns=poly_feature_names, index=df_temp.index)\n",
    "        poly_df = poly_df.drop(columns=[col for col in poly_df.columns if col in df_temp.columns])\n",
    "        new_features_df = pd.concat([new_features_df, poly_df], axis=1)\n",
    "        print(f\"✅ Added {poly_df.shape[1]} interaction/polynomial features.\")\n",
    "\n",
    "    # --- Moving Average and Lag Features ---\n",
    "    if datetime_col and datetime_col in df_temp.columns:\n",
    "        print(\"\\n🕒 Creating moving average and lag features...\")\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df_temp[datetime_col]):\n",
    "            df_temp[datetime_col] = pd.to_datetime(df_temp[datetime_col], errors='coerce')\n",
    "        df_temp = df_temp.sort_values(by=datetime_col)\n",
    "\n",
    "        for col in numerical_cols:\n",
    "            new_features_df[f\"{col}_ma3\"] = df_temp[col].rolling(window=3, min_periods=1).mean().values\n",
    "            new_features_df[f\"{col}_lag1\"] = df_temp[col].shift(1).values\n",
    "        print(f\"✅ Added moving averages & lags for: {numerical_cols}\")\n",
    "\n",
    "    # --- Group-wise Aggregates ---\n",
    "    if cat_cols:\n",
    "        print(\"\\n🔄 Generating group-wise aggregate features...\")\n",
    "        for cat in cat_cols:\n",
    "            for num in numerical_cols:\n",
    "                new_features_df[f\"{cat}_{num}_mean\"] = df_temp.groupby(cat)[num].transform('mean')\n",
    "                new_features_df[f\"{cat}_{num}_std\"] = df_temp.groupby(cat)[num].transform('std')\n",
    "        print(f\"✅ Aggregates created for categorical-numerical combinations.\")\n",
    "\n",
    "    print(\"\\n🧠 Mathematical feature engineering complete.\")\n",
    "    return new_features_df\n",
    "    \n",
    "new_math_features = step16_mathematical_feature_engineering(df, datetime_col='timestap')\n",
    "new_math_features.head()\n",
    "# df remains unchanged and new_math_features contains only the new engineered columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c7b07bd5-8a53-4b5f-8714-63b7008ca176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked_missing</th>\n",
       "      <th>Age_missing</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_q</th>\n",
       "      <th>Embarked_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a/5 21171</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.692582</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pc 17599</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.101506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ston/o2. 3101282</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.815138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>113803</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.286974</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>373450</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.837252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Ticket  Pclass   Age  Parch      Fare  Embarked_missing  \\\n",
       "0         a/5 21171       3  22.0    0.0  2.692582               0.0   \n",
       "1          pc 17599       1  38.0    0.0  8.101506               0.0   \n",
       "2  ston/o2. 3101282       3  26.0    0.0  2.815138               0.0   \n",
       "3            113803       1  35.0    0.0  7.286974               0.0   \n",
       "4            373450       3  35.0    0.0  2.837252               0.0   \n",
       "\n",
       "   Age_missing  Sex_male  Embarked_q  Embarked_s  \n",
       "0          0.0      True       False        True  \n",
       "1          0.0     False       False       False  \n",
       "2          0.0     False       False        True  \n",
       "3          0.0     False       False        True  \n",
       "4          0.0      True       False        True  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b26b034c-4c0b-43b7-852d-bfca8798c7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧮 Generated 12 statistical features.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass_zscore</th>\n",
       "      <th>Age_zscore</th>\n",
       "      <th>Parch_zscore</th>\n",
       "      <th>Fare_zscore</th>\n",
       "      <th>Embarked_missing_zscore</th>\n",
       "      <th>Age_missing_zscore</th>\n",
       "      <th>Pclass_percentile_Ticket</th>\n",
       "      <th>Age_percentile_Ticket</th>\n",
       "      <th>Parch_percentile_Ticket</th>\n",
       "      <th>Fare_percentile_Ticket</th>\n",
       "      <th>Embarked_missing_percentile_Ticket</th>\n",
       "      <th>Age_missing_percentile_Ticket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.827377</td>\n",
       "      <td>-0.611917</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.904305</td>\n",
       "      <td>-0.047424</td>\n",
       "      <td>-0.497803</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.566107</td>\n",
       "      <td>0.715304</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.824259</td>\n",
       "      <td>-0.047424</td>\n",
       "      <td>-0.497803</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.827377</td>\n",
       "      <td>-0.280112</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.842482</td>\n",
       "      <td>-0.047424</td>\n",
       "      <td>-0.497803</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.566107</td>\n",
       "      <td>0.466450</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.413364</td>\n",
       "      <td>-0.047424</td>\n",
       "      <td>-0.497803</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.827377</td>\n",
       "      <td>0.466450</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.831326</td>\n",
       "      <td>-0.047424</td>\n",
       "      <td>-0.497803</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass_zscore  Age_zscore  Parch_zscore  Fare_zscore  \\\n",
       "0       0.827377   -0.611917           NaN    -0.904305   \n",
       "1      -1.566107    0.715304           NaN     1.824259   \n",
       "2       0.827377   -0.280112           NaN    -0.842482   \n",
       "3      -1.566107    0.466450           NaN     1.413364   \n",
       "4       0.827377    0.466450           NaN    -0.831326   \n",
       "\n",
       "   Embarked_missing_zscore  Age_missing_zscore  Pclass_percentile_Ticket  \\\n",
       "0                -0.047424           -0.497803                      1.00   \n",
       "1                -0.047424           -0.497803                      1.00   \n",
       "2                -0.047424           -0.497803                      1.00   \n",
       "3                -0.047424           -0.497803                      0.75   \n",
       "4                -0.047424           -0.497803                      1.00   \n",
       "\n",
       "   Age_percentile_Ticket  Parch_percentile_Ticket  Fare_percentile_Ticket  \\\n",
       "0                    1.0                     1.00                    1.00   \n",
       "1                    1.0                     1.00                    1.00   \n",
       "2                    1.0                     1.00                    1.00   \n",
       "3                    0.5                     0.75                    0.75   \n",
       "4                    1.0                     1.00                    1.00   \n",
       "\n",
       "   Embarked_missing_percentile_Ticket  Age_missing_percentile_Ticket  \n",
       "0                                1.00                           1.00  \n",
       "1                                1.00                           1.00  \n",
       "2                                1.00                           1.00  \n",
       "3                                0.75                           0.75  \n",
       "4                                1.00                           1.00  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import zscore\n",
    "import pandas as pd\n",
    "\n",
    "def step17_statistical_feature_engineering(df, datetime_col=None):\n",
    "    \"\"\"\n",
    "    Generates statistical features without modifying the original DataFrame.\n",
    "    \n",
    "    Includes:\n",
    "    - Z-scores\n",
    "    - Percentile ranks within groups\n",
    "    - Rolling mean deviation and volatility (if datetime provided)\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        datetime_col (str): Optional datetime column for time-based calculations\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: New DataFrame containing only the generated statistical features\n",
    "    \"\"\"\n",
    "    df_temp = df.copy()\n",
    "    df_stats = pd.DataFrame(index=df_temp.index)\n",
    "\n",
    "    numerical_cols = df_temp.select_dtypes(include='number').columns.tolist()\n",
    "    group_cols = df_temp.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    # 1️⃣ Z-scores\n",
    "    for col in numerical_cols:\n",
    "        df_stats[f'{col}_zscore'] = zscore(df_temp[col].fillna(df_temp[col].mean()))\n",
    "\n",
    "    # 2️⃣ Percentile ranks within group columns\n",
    "    if group_cols:\n",
    "        for group in group_cols:\n",
    "            for col in numerical_cols:\n",
    "                df_stats[f'{col}_percentile_{group}'] = df_temp.groupby(group)[col].rank(pct=True)\n",
    "\n",
    "    # 3️⃣ Rolling mean, deviation & volatility\n",
    "    if datetime_col and datetime_col in df_temp.columns:\n",
    "        df_temp[datetime_col] = pd.to_datetime(df_temp[datetime_col], errors='coerce')\n",
    "        df_temp = df_temp.sort_values(by=datetime_col)\n",
    "\n",
    "        for col in numerical_cols:\n",
    "            rolling_mean = df_temp[col].rolling(window=5, min_periods=1).mean()\n",
    "            rolling_std = df_temp[col].rolling(window=5, min_periods=1).std()\n",
    "            rolling_dev = df_temp[col] - rolling_mean\n",
    "\n",
    "            df_stats[f'{col}_rolling_mean_5'] = rolling_mean\n",
    "            df_stats[f'{col}_rolling_dev'] = rolling_dev\n",
    "            df_stats[f'{col}_volatility'] = rolling_std\n",
    "\n",
    "    print(f\"\\n🧮 Generated {df_stats.shape[1]} statistical features.\")\n",
    "    return df_stats\n",
    "    \n",
    "new_stat_features = step17_statistical_feature_engineering(df, datetime_col='timestamp')\n",
    "new_stat_features.head()\n",
    "# df stays intact, and new_stat_features contains only the engineered statistical features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5ef63e0d-582c-4d4e-8876-03431a5fa688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked_missing</th>\n",
       "      <th>Age_missing</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_q</th>\n",
       "      <th>Embarked_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a/5 21171</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.692582</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pc 17599</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.101506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ston/o2. 3101282</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.815138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>113803</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.286974</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>373450</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.837252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Ticket  Pclass   Age  Parch      Fare  Embarked_missing  \\\n",
       "0         a/5 21171       3  22.0    0.0  2.692582               0.0   \n",
       "1          pc 17599       1  38.0    0.0  8.101506               0.0   \n",
       "2  ston/o2. 3101282       3  26.0    0.0  2.815138               0.0   \n",
       "3            113803       1  35.0    0.0  7.286974               0.0   \n",
       "4            373450       3  35.0    0.0  2.837252               0.0   \n",
       "\n",
       "   Age_missing  Sex_male  Embarked_q  Embarked_s  \n",
       "0          0.0      True       False        True  \n",
       "1          0.0     False       False       False  \n",
       "2          0.0     False       False        True  \n",
       "3          0.0     False       False        True  \n",
       "4          0.0      True       False        True  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9383a525-da96-47e1-b311-c40fc974d665",
   "metadata": {},
   "source": [
    "Phase 2.4: Advanced Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aee15109-4303-41d7-adae-951044e23b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose method (pca, tsne, umap, autoencoder):  pca\n",
      "Add cluster-based features? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧮 Applying PCA...\n",
      "\n",
      "📌 PCA Component Weights:\n",
      "     Pclass     Age  Parch    Fare  Embarked_missing  Age_missing  Sex_male  \\\n",
      "PC1  0.5830 -0.2380   -0.0 -0.5499           -0.1260       0.3259    0.1688   \n",
      "PC2  0.2339 -0.1497   -0.0 -0.2766           -0.0525      -0.3379    0.2764   \n",
      "\n",
      "     Embarked_q  Embarked_s  \n",
      "PC1      0.3665     -0.1272  \n",
      "PC2     -0.5120      0.6252  \n",
      "\n",
      "📊 Applying KMeans Clustering...\n",
      "✅ Optimal Clusters: 2 with Silhouette Score: 0.3931\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pca_1</th>\n",
       "      <th>pca_2</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.902406</td>\n",
       "      <td>1.452358</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.377642</td>\n",
       "      <td>-2.039552</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.436028</td>\n",
       "      <td>0.807077</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.377436</td>\n",
       "      <td>-0.488442</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.605643</td>\n",
       "      <td>1.270767</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pca_1     pca_2  cluster\n",
       "0  0.902406  1.452358        0\n",
       "1 -2.377642 -2.039552        0\n",
       "2  0.436028  0.807077        0\n",
       "3 -2.377436 -0.488442        0\n",
       "4  0.605643  1.270767        0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def step18_dimensionality_reduction(df, method='pca', n_components=2, use_clusters=False):\n",
    "    \"\"\"\n",
    "    Applies dimensionality reduction and optionally adds clustering.\n",
    "\n",
    "    Returns a new DataFrame containing only the generated features,\n",
    "    preserving the original df.\n",
    "    \"\"\"\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.manifold import TSNE\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    from pandas.api.types import is_numeric_dtype, is_bool_dtype\n",
    "\n",
    "    # Optional UMAP + Autoencoder dependencies\n",
    "    try:\n",
    "        import umap\n",
    "        umap_available = True\n",
    "    except ImportError:\n",
    "        umap_available = False\n",
    "\n",
    "    try:\n",
    "        from keras.models import Model\n",
    "        from keras.layers import Input, Dense\n",
    "        from keras.optimizers import Adam\n",
    "        autoencoder_available = True\n",
    "    except ImportError:\n",
    "        autoencoder_available = False\n",
    "\n",
    "    df = df.copy()\n",
    "    X = df.select_dtypes(include=['number', 'bool']).copy()\n",
    "\n",
    "    # Robust missing value handling\n",
    "    for col in X.columns:\n",
    "        if is_numeric_dtype(X[col]):\n",
    "            median_val = X[col].median()\n",
    "            X[col] = X[col].fillna(0 if pd.isna(median_val) else median_val)\n",
    "        elif is_bool_dtype(X[col]):\n",
    "            X[col] = X[col].fillna(False)\n",
    "        else:\n",
    "            mode_series = X[col].mode()\n",
    "            X[col] = X[col].fillna(mode_series[0] if not mode_series.empty else \"missing\")\n",
    "\n",
    "    if X.isnull().values.any():\n",
    "        raise ValueError(\"🛑 NaNs still exist after imputation.\")\n",
    "\n",
    "    # Scale the numeric values\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    df_new = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # Dimensionality Reduction\n",
    "    if method == 'pca':\n",
    "        print(\"\\n🧮 Applying PCA...\")\n",
    "        model = PCA(n_components=n_components)\n",
    "        reduced = model.fit_transform(X_scaled)\n",
    "        df_new[[f'pca_{i+1}' for i in range(n_components)]] = reduced\n",
    "\n",
    "        pca_weights = pd.DataFrame(model.components_, columns=X.columns, index=[f'PC{i+1}' for i in range(n_components)])\n",
    "        print(\"\\n📌 PCA Component Weights:\")\n",
    "        print(pca_weights.round(4))\n",
    "\n",
    "    elif method == 'tsne':\n",
    "        print(\"\\n🌌 Applying t-SNE...\")\n",
    "        model = TSNE(n_components=n_components, random_state=42)\n",
    "        reduced = model.fit_transform(X_scaled)\n",
    "        df_new[[f'tsne_{i+1}' for i in range(n_components)]] = reduced\n",
    "\n",
    "    elif method == 'umap':\n",
    "        if not umap_available:\n",
    "            print(\"❌ UMAP not installed.\")\n",
    "        else:\n",
    "            print(\"\\n🧭 Applying UMAP...\")\n",
    "            model = umap.UMAP(n_components=n_components, random_state=42)\n",
    "            reduced = model.fit_transform(X_scaled)\n",
    "            df_new[[f'umap_{i+1}' for i in range(n_components)]] = reduced\n",
    "\n",
    "    elif method == 'autoencoder':\n",
    "        if not autoencoder_available:\n",
    "            print(\"❌ Autoencoder dependencies not installed.\")\n",
    "        else:\n",
    "            print(\"\\n🤖 Applying Autoencoder...\")\n",
    "            input_dim = X_scaled.shape[1]\n",
    "            input_layer = Input(shape=(input_dim,))\n",
    "            encoded = Dense(64, activation='relu')(input_layer)\n",
    "            encoded = Dense(n_components, activation='relu')(encoded)\n",
    "            decoded = Dense(64, activation='relu')(encoded)\n",
    "            output_layer = Dense(input_dim, activation='linear')(decoded)\n",
    "\n",
    "            autoencoder = Model(input_layer, output_layer)\n",
    "            encoder = Model(input_layer, encoded)\n",
    "            autoencoder.compile(optimizer=Adam(0.01), loss='mse')\n",
    "            autoencoder.fit(X_scaled, X_scaled, epochs=30, batch_size=32, verbose=0)\n",
    "\n",
    "            encoded_features = encoder.predict(X_scaled)\n",
    "            df_new[[f'autoenc_{i+1}' for i in range(n_components)]] = encoded_features\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"❌ Unknown method: {method}\")\n",
    "\n",
    "    # Clustering\n",
    "    if use_clusters:\n",
    "        print(\"\\n📊 Applying KMeans Clustering...\")\n",
    "        best_k, best_score = 2, -1\n",
    "        for k in range(2, min(10, len(X_scaled))):\n",
    "            try:\n",
    "                kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "                labels = kmeans.fit_predict(X_scaled)\n",
    "                score = silhouette_score(X_scaled, labels)\n",
    "                if score > best_score:\n",
    "                    best_score, best_k = score, k\n",
    "            except Exception:\n",
    "                continue\n",
    "        print(f\"✅ Optimal Clusters: {best_k} with Silhouette Score: {round(best_score, 4)}\")\n",
    "        df_new['cluster'] = KMeans(n_clusters=best_k, random_state=42).fit_predict(X_scaled)\n",
    "\n",
    "    return df_new\n",
    "\n",
    "method = input(\"Choose method (pca, tsne, umap, autoencoder): \").strip().lower()\n",
    "use_clusters = input(\"Add cluster-based features? (y/n): \").strip().lower() == 'y'\n",
    "\n",
    "reduced_df = step18_dimensionality_reduction(df, method=method, n_components=2, use_clusters=use_clusters)\n",
    "reduced_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b48e678f-505c-4ff3-aefe-b0045ee80c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked_missing</th>\n",
       "      <th>Age_missing</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_q</th>\n",
       "      <th>Embarked_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a/5 21171</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.692582</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pc 17599</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.101506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ston/o2. 3101282</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.815138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>113803</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.286974</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>373450</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.837252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Ticket  Pclass   Age  Parch      Fare  Embarked_missing  \\\n",
       "0         a/5 21171       3  22.0    0.0  2.692582               0.0   \n",
       "1          pc 17599       1  38.0    0.0  8.101506               0.0   \n",
       "2  ston/o2. 3101282       3  26.0    0.0  2.815138               0.0   \n",
       "3            113803       1  35.0    0.0  7.286974               0.0   \n",
       "4            373450       3  35.0    0.0  2.837252               0.0   \n",
       "\n",
       "   Age_missing  Sex_male  Embarked_q  Embarked_s  \n",
       "0          0.0      True       False        True  \n",
       "1          0.0     False       False       False  \n",
       "2          0.0     False       False        True  \n",
       "3          0.0     False       False        True  \n",
       "4          0.0      True       False        True  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2ed651-2aa8-46d4-983f-4b89c75ea2c1",
   "metadata": {},
   "source": [
    "Stage 3: Advanced Feature Selection (Sophisticated Selection Methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c377488-036a-4ac7-8ac4-a126dcc2165a",
   "metadata": {},
   "source": [
    "Phase 3.1: Filter-Based Selection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "123c0e34-aa50-46ab-ad8f-20f6c9b933fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Step 20: Feature Relationship Summary & Ranking\n",
      "\n",
      "🔗 Mutual Information (non-linear relation)...\n",
      "\n",
      "📈 F-statistic (linear relation - numerical)...\n",
      "\n",
      "📊 Chi-Square (categorical relation)...\n",
      "\n",
      "🧩 Feature Relationship Report:\n",
      " • Ticket → Categorical relationship (MI: 0.115, F: nan, Chi2: 2873.421)\n",
      " • Fare → Linear relationship (MI: 0.122, F: 105.694, Chi2: nan)\n",
      " • pca_1 → Linear relationship (MI: 0.107, F: 106.394, Chi2: nan)\n",
      " • Pclass → Linear relationship (MI: 0.087, F: 115.031, Chi2: nan)\n",
      " • pca_2 → Linear relationship (MI: 0.132, F: 82.637, Chi2: nan)\n",
      " • Sex_male → Non-Linear relationship (MI: 0.171, F: nan, Chi2: 92.702)\n",
      " • Age → Weak/No Clear Relationship relationship (MI: 0.024, F: 3.873, Chi2: nan)\n",
      " • Embarked_s → Weak/No Clear Relationship relationship (MI: 0.024, F: nan, Chi2: 5.489)\n",
      " • Embarked_missing → Weak/No Clear Relationship relationship (MI: 0.012, F: 3.222, Chi2: nan)\n",
      " • Age_missing → Weak/No Clear Relationship relationship (MI: 0.000, F: 7.621, Chi2: nan)\n",
      " • Embarked_q → Weak/No Clear Relationship relationship (MI: 0.010, F: nan, Chi2: 0.011)\n",
      " • Parch → Weak/No Clear Relationship relationship (MI: 0.004, F: nan, Chi2: nan)\n",
      " • cluster → Weak/No Clear Relationship relationship (MI: 0.000, F: 0.012, Chi2: nan)\n"
     ]
    }
   ],
   "source": [
    "def step20_feature_relationship_summary(df, y, numerical_cols, categorical_cols):\n",
    "    from sklearn.feature_selection import mutual_info_classif, f_classif, chi2\n",
    "    from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    print(\"\\n📊 Step 20: Feature Relationship Summary & Ranking\")\n",
    "\n",
    "    feature_scores = pd.DataFrame()\n",
    "    feature_scores['feature'] = numerical_cols + categorical_cols\n",
    "\n",
    "    # Encode target\n",
    "    if y.dtype == 'object' or y.dtype.name == 'category':\n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "    # Select and clean features\n",
    "    existing_cols = [col for col in numerical_cols + categorical_cols if col in df.columns]\n",
    "    X_all = df[existing_cols].copy()\n",
    "\n",
    "    # Fill numerical NaNs\n",
    "    for col in numerical_cols:\n",
    "        if col in X_all.columns:\n",
    "            median = X_all[col].median()\n",
    "            X_all[col] = X_all[col].fillna(0 if pd.isna(median) else median)\n",
    "\n",
    "    # Fill and encode categorical NaNs\n",
    "    for col in categorical_cols:\n",
    "        if col in X_all.columns:\n",
    "            X_all[col] = X_all[col].astype(str).fillna(\"missing\")\n",
    "            X_all[col] = LabelEncoder().fit_transform(X_all[col])\n",
    "\n",
    "    # --- Mutual Info (all)\n",
    "    print(\"\\n🔗 Mutual Information (non-linear relation)...\")\n",
    "    mutual_info = mutual_info_classif(X_all, y, discrete_features='auto')\n",
    "    feature_scores['mutual_info'] = mutual_info\n",
    "\n",
    "    # --- F-statistic (numerical only)\n",
    "    print(\"\\n📈 F-statistic (linear relation - numerical)...\")\n",
    "    f_vals = np.full(len(feature_scores), np.nan)\n",
    "    if numerical_cols:\n",
    "        X_num = df[numerical_cols].copy()\n",
    "        for col in X_num.columns:\n",
    "            median = X_num[col].median()\n",
    "            X_num[col] = X_num[col].fillna(0 if pd.isna(median) else median)\n",
    "\n",
    "        f_stat_vals, _ = f_classif(X_num, y)\n",
    "        for i, col in enumerate(numerical_cols):\n",
    "            f_vals[feature_scores['feature'] == col] = f_stat_vals[i]\n",
    "    feature_scores['f_stat'] = f_vals\n",
    "\n",
    "    # --- Chi-Square (categorical only)\n",
    "    print(\"\\n📊 Chi-Square (categorical relation)...\")\n",
    "    chi_vals = np.full(len(feature_scores), np.nan)\n",
    "    if categorical_cols:\n",
    "        encoded_cat = df[categorical_cols].copy()\n",
    "        for col in encoded_cat.columns:\n",
    "            encoded_cat[col] = encoded_cat[col].astype(str).fillna(\"missing\")\n",
    "            encoded_cat[col] = LabelEncoder().fit_transform(encoded_cat[col])\n",
    "        chi_vals_raw, _ = chi2(encoded_cat, y)\n",
    "        for i, col in enumerate(categorical_cols):\n",
    "            chi_vals[feature_scores['feature'] == col] = chi_vals_raw[i]\n",
    "    feature_scores['chi2'] = chi_vals\n",
    "\n",
    "    # --- Normalize scores\n",
    "    scaler = MinMaxScaler()\n",
    "    for col in ['mutual_info', 'f_stat', 'chi2']:\n",
    "        feature_scores[col + '_scaled'] = scaler.fit_transform(feature_scores[[col]].fillna(0))\n",
    "\n",
    "    # --- Detect relation type\n",
    "    def detect_relation(row):\n",
    "        if row['f_stat_scaled'] > 0.7:\n",
    "            return 'Linear'\n",
    "        elif row['chi2_scaled'] > 0.7:\n",
    "            return 'Categorical'\n",
    "        elif row['mutual_info_scaled'] > 0.7:\n",
    "            return 'Non-Linear'\n",
    "        else:\n",
    "            return 'Weak/No Clear Relationship'\n",
    "\n",
    "    feature_scores['relation_type'] = feature_scores.apply(detect_relation, axis=1)\n",
    "\n",
    "    # --- Composite Score\n",
    "    feature_scores['composite_score'] = feature_scores[\n",
    "        ['mutual_info_scaled', 'f_stat_scaled', 'chi2_scaled']\n",
    "    ].mean(axis=1)\n",
    "\n",
    "    feature_scores.sort_values(\"composite_score\", ascending=False, inplace=True)\n",
    "\n",
    "    # --- Print Report\n",
    "    print(\"\\n🧩 Feature Relationship Report:\")\n",
    "    for _, row in feature_scores.iterrows():\n",
    "        print(f\" • {row['feature']} → {row['relation_type']} relationship \"\n",
    "              f\"(MI: {row['mutual_info']:.3f}, F: {row['f_stat']:.3f}, Chi2: {row['chi2']:.3f})\")\n",
    "\n",
    "    return feature_scores\n",
    "\n",
    "numerical_cols = df.select_dtypes(include='number').columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n",
    "ranking_df = step20_feature_relationship_summary(df, y, numerical_cols, categorical_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45ac8c6-e870-49e2-8ed5-c2133ec8e285",
   "metadata": {},
   "source": [
    "Phase 3.2: Wrapper-Based Selection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d9dbffc0-4446-4004-876b-684a9e8927d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Step 23: Backward Feature Elimination (RFECV)\n",
      "📌 Starting with all features and evaluating removals...\n",
      "⚠️ Dropped 1 constant/all-NaN numeric columns: ['Parch']\n",
      "🔁 Generating interaction features (degree=2) using first 20 columns...\n",
      "🔢 Total features considered: 881\n",
      "\n",
      "✅ Final Selected Features (203):\n",
      "   - Pclass\n",
      "   - Age\n",
      "   - Fare\n",
      "   - pca_1\n",
      "   - pca_2\n",
      "   - Sex_male\n",
      "   - Pclass Age\n",
      "   - Pclass pca_1\n",
      "   - Pclass Sex_male\n",
      "   - Pclass Ticket_110465\n",
      "   - Pclass Ticket_110564\n",
      "   - Pclass Ticket_111369\n",
      "   - Pclass Ticket_111426\n",
      "   - Age Age_missing\n",
      "   - Age pca_1\n",
      "   - Age cluster\n",
      "   - Age Ticket_110413\n",
      "   - Fare pca_1\n",
      "   - Fare Sex_male\n",
      "   - Fare Embarked_s\n",
      "   - pca_1 Embarked_s\n",
      "   - pca_2 Sex_male\n",
      "   - Sex_male Embarked_s\n",
      "   - Sex_male Ticket_111426\n",
      "   - Ticket_111427\n",
      "   - Ticket_111428\n",
      "   - Ticket_112277\n",
      "   - Ticket_113051\n",
      "   - Ticket_113055\n",
      "   - Ticket_113059\n",
      "   - Ticket_113501\n",
      "   - Ticket_113503\n",
      "   - Ticket_113760\n",
      "   - Ticket_113773\n",
      "   - Ticket_113781\n",
      "   - Ticket_113786\n",
      "   - Ticket_113788\n",
      "   - Ticket_113794\n",
      "   - Ticket_113804\n",
      "   - Ticket_113806\n",
      "   - Ticket_11668\n",
      "   - Ticket_11751\n",
      "   - Ticket_11753\n",
      "   - Ticket_11765\n",
      "   - Ticket_11774\n",
      "   - Ticket_11967\n",
      "   - Ticket_13213\n",
      "   - Ticket_13214\n",
      "   - Ticket_13567\n",
      "   - Ticket_1601\n",
      "   - Ticket_16988\n",
      "   - Ticket_17453\n",
      "   - Ticket_17474\n",
      "   - Ticket_19943\n",
      "   - Ticket_19947\n",
      "   - Ticket_19950\n",
      "   - Ticket_19952\n",
      "   - Ticket_19988\n",
      "   - Ticket_19996\n",
      "   - Ticket_220845\n",
      "   - Ticket_230080\n",
      "   - Ticket_230136\n",
      "   - Ticket_237671\n",
      "   - Ticket_237798\n",
      "   - Ticket_239865\n",
      "   - Ticket_244252\n",
      "   - Ticket_244270\n",
      "   - Ticket_244373\n",
      "   - Ticket_248698\n",
      "   - Ticket_248738\n",
      "   - Ticket_248747\n",
      "   - Ticket_250651\n",
      "   - Ticket_2620\n",
      "   - Ticket_2627\n",
      "   - Ticket_2650\n",
      "   - Ticket_2651\n",
      "   - Ticket_2653\n",
      "   - Ticket_2661\n",
      "   - Ticket_2663\n",
      "   - Ticket_2665\n",
      "   - Ticket_2666\n",
      "   - Ticket_2668\n",
      "   - Ticket_2677\n",
      "   - Ticket_2678\n",
      "   - Ticket_2689\n",
      "   - Ticket_2691\n",
      "   - Ticket_27042\n",
      "   - Ticket_2908\n",
      "   - Ticket_29106\n",
      "   - Ticket_3101265\n",
      "   - Ticket_3101278\n",
      "   - Ticket_3101281\n",
      "   - Ticket_3101295\n",
      "   - Ticket_312991\n",
      "   - Ticket_315084\n",
      "   - Ticket_315096\n",
      "   - Ticket_315098\n",
      "   - Ticket_315153\n",
      "   - Ticket_330909\n",
      "   - Ticket_330935\n",
      "   - Ticket_343120\n",
      "   - Ticket_345572\n",
      "   - Ticket_345764\n",
      "   - Ticket_345773\n",
      "   - Ticket_345774\n",
      "   - Ticket_345779\n",
      "   - Ticket_347054\n",
      "   - Ticket_347071\n",
      "   - Ticket_347077\n",
      "   - Ticket_347080\n",
      "   - Ticket_347081\n",
      "   - Ticket_347082\n",
      "   - Ticket_347083\n",
      "   - Ticket_347085\n",
      "   - Ticket_347087\n",
      "   - Ticket_347088\n",
      "   - Ticket_347089\n",
      "   - Ticket_347470\n",
      "   - Ticket_347742\n",
      "   - Ticket_349236\n",
      "   - Ticket_349237\n",
      "   - Ticket_349240\n",
      "   - Ticket_349244\n",
      "   - Ticket_349245\n",
      "   - Ticket_349909\n",
      "   - Ticket_350034\n",
      "   - Ticket_350043\n",
      "   - Ticket_350046\n",
      "   - Ticket_350406\n",
      "   - Ticket_350417\n",
      "   - Ticket_35281\n",
      "   - Ticket_363291\n",
      "   - Ticket_364516\n",
      "   - Ticket_364846\n",
      "   - Ticket_364848\n",
      "   - Ticket_364849\n",
      "   - Ticket_364850\n",
      "   - Ticket_365226\n",
      "   - Ticket_367226\n",
      "   - Ticket_367228\n",
      "   - Ticket_370129\n",
      "   - Ticket_376564\n",
      "   - Ticket_382649\n",
      "   - Ticket_382651\n",
      "   - Ticket_382652\n",
      "   - Ticket_386525\n",
      "   - Ticket_392091\n",
      "   - Ticket_392096\n",
      "   - Ticket_4133\n",
      "   - Ticket_4134\n",
      "   - Ticket_4136\n",
      "   - Ticket_4137\n",
      "   - Ticket_4138\n",
      "   - Ticket_65306\n",
      "   - Ticket_7552\n",
      "   - Ticket_7553\n",
      "   - Ticket_7598\n",
      "   - Ticket_a/5 3540\n",
      "   - Ticket_a/5. 10482\n",
      "   - Ticket_a/5. 3336\n",
      "   - Ticket_c 17369\n",
      "   - Ticket_c 7077\n",
      "   - Ticket_c.a. 2315\n",
      "   - Ticket_c.a. 2673\n",
      "   - Ticket_c.a. 37671\n",
      "   - Ticket_ca 2144\n",
      "   - Ticket_ca. 2314\n",
      "   - Ticket_ca. 2343\n",
      "   - Ticket_line\n",
      "   - Ticket_pc 17473\n",
      "   - Ticket_pc 17474\n",
      "   - Ticket_pc 17475\n",
      "   - Ticket_pc 17476\n",
      "   - Ticket_pc 17485\n",
      "   - Ticket_pc 17558\n",
      "   - Ticket_pc 17572\n",
      "   - Ticket_pc 17593\n",
      "   - Ticket_pc 17595\n",
      "   - Ticket_pc 17604\n",
      "   - Ticket_pc 17611\n",
      "   - Ticket_pc 17755\n",
      "   - Ticket_pc 17758\n",
      "   - Ticket_pc 17759\n",
      "   - Ticket_pc 17760\n",
      "   - Ticket_pp 9549\n",
      "   - Ticket_s.o./p.p. 3\n",
      "   - Ticket_s.o.c. 14879\n",
      "   - Ticket_s.w./pp 752\n",
      "   - Ticket_sc/paris 2146\n",
      "   - Ticket_soton/o.q. 392078\n",
      "   - Ticket_soton/oq 392089\n",
      "   - Ticket_ston/o 2. 3101269\n",
      "   - Ticket_ston/o 2. 3101285\n",
      "   - Ticket_ston/o 2. 3101286\n",
      "   - Ticket_ston/o 2. 3101288\n",
      "   - Ticket_ston/o 2. 3101289\n",
      "   - Ticket_ston/o2. 3101271\n",
      "   - Ticket_ston/o2. 3101279\n",
      "   - Ticket_ston/o2. 3101282\n",
      "   - Ticket_ston/o2. 3101283\n",
      "   - Ticket_ston/o2. 3101290\n",
      "   - Ticket_sw/pp 751\n",
      "   - Ticket_w./c. 6608\n",
      "\n",
      "📊 Feature Ranking Summary:\n",
      "                       Feature  Ranking  Selected\n",
      "                        Pclass        1      True\n",
      "                    Pclass Age        1      True\n",
      "                    Fare pca_1        1      True\n",
      "             Ticket_w./c. 6608        1      True\n",
      "                 Fare Sex_male        1      True\n",
      "           Ticket_s.o.c. 14879        1      True\n",
      "      Ticket_soton/o.q. 392078        1      True\n",
      "               Fare Embarked_s        1      True\n",
      "                Ticket_3101278        1      True\n",
      "                Ticket_3101281        1      True\n",
      "            Ticket_s.o./p.p. 3        1      True\n",
      "                 Ticket_376564        1      True\n",
      "                 Ticket_237798        1      True\n",
      "                 Ticket_237671        1      True\n",
      "                Ticket_3101295        1      True\n",
      "                 Ticket_370129        1      True\n",
      "                  Pclass pca_1        1      True\n",
      "               Pclass Sex_male        1      True\n",
      "                 Ticket_312991        1      True\n",
      "                 Ticket_315084        1      True\n",
      "            Ticket_s.w./pp 752        1      True\n",
      "                 Ticket_382649        1      True\n",
      "                 Ticket_382651        1      True\n",
      "                 Ticket_382652        1      True\n",
      "                 Ticket_350043        1      True\n",
      "                 Ticket_350034        1      True\n",
      "                 Ticket_350046        1      True\n",
      "                 Ticket_347071        1      True\n",
      "                   Ticket_2908        1      True\n",
      "                  Ticket_35281        1      True\n",
      "                 Ticket_350417        1      True\n",
      "                 Ticket_350406        1      True\n",
      "               Ticket_pc 17755        1      True\n",
      "               Ticket_pc 17758        1      True\n",
      "                 Ticket_230136        1      True\n",
      "                  Ticket_29106        1      True\n",
      "                Ticket_3101265        1      True\n",
      "                 Ticket_347054        1      True\n",
      "                 Ticket_239865        1      True\n",
      "                 Ticket_347077        1      True\n",
      "                  Ticket_19950        1      True\n",
      "                  Ticket_19952        1      True\n",
      "                 Ticket_244252        1      True\n",
      "                 Ticket_244270        1      True\n",
      "                  Ticket_19988        1      True\n",
      "                  Ticket_19996        1      True\n",
      "                 Ticket_230080        1      True\n",
      "                 Ticket_220845        1      True\n",
      "                Ticket_pp 9549        1      True\n",
      "                pca_2 Sex_male        1      True\n",
      "               Ticket_pc 17759        1      True\n",
      "               Ticket_pc 17760        1      True\n",
      "                  Ticket_27042        1      True\n",
      "          Ticket_sc/paris 2146        1      True\n",
      "              pca_1 Embarked_s        1      True\n",
      "               Ticket_pc 17611        1      True\n",
      "                  Ticket_16988        1      True\n",
      "                   Ticket_1601        1      True\n",
      "                  Ticket_13567        1      True\n",
      "                 Ticket_347742        1      True\n",
      "                 Ticket_347470        1      True\n",
      "                 Ticket_330935        1      True\n",
      "                 Ticket_330909        1      True\n",
      "                 Ticket_315153        1      True\n",
      "                 Ticket_364849        1      True\n",
      "               Ticket_pc 17474        1      True\n",
      "               Ticket_pc 17475        1      True\n",
      "                   Ticket_line        1      True\n",
      "                 Ticket_367226        1      True\n",
      "                 Ticket_345779        1      True\n",
      "                 Ticket_367228        1      True\n",
      "                 Ticket_347080        1      True\n",
      "                  Ticket_17453        1      True\n",
      "                  Ticket_17474        1      True\n",
      "                  Ticket_19943        1      True\n",
      "                  Ticket_19947        1      True\n",
      "                 Ticket_386525        1      True\n",
      "                 Ticket_392091        1      True\n",
      "                 Ticket_392096        1      True\n",
      "                   Ticket_4133        1      True\n",
      "               Ticket_pc 17558        1      True\n",
      "               Ticket_pc 17572        1      True\n",
      "                 Ticket_349244        1      True\n",
      "                 Ticket_349245        1      True\n",
      "                 Ticket_349909        1      True\n",
      "               Ticket_pc 17593        1      True\n",
      "               Ticket_pc 17595        1      True\n",
      "                   Ticket_2666        1      True\n",
      "                   Ticket_2689        1      True\n",
      "                   Ticket_2668        1      True\n",
      "                   Ticket_2677        1      True\n",
      "                   Ticket_2678        1      True\n",
      "                   Ticket_2691        1      True\n",
      "        Sex_male Ticket_111426        1      True\n",
      "           Sex_male Embarked_s        1      True\n",
      "               Ticket_pc 17604        1      True\n",
      "                 Ticket_349240        1      True\n",
      "                 Ticket_364850        1      True\n",
      "                 Ticket_365226        1      True\n",
      "               Ticket_pc 17476        1      True\n",
      "               Ticket_pc 17485        1      True\n",
      "                 Ticket_364516        1      True\n",
      "                 Ticket_364846        1      True\n",
      "                 Ticket_364848        1      True\n",
      "                 Ticket_347085        1      True\n",
      "                 Ticket_347081        1      True\n",
      "                 Ticket_347082        1      True\n",
      "                 Ticket_347083        1      True\n",
      "                 Ticket_347088        1      True\n",
      "                 Ticket_347087        1      True\n",
      "                 Ticket_347089        1      True\n",
      "               Ticket_pc 17473        1      True\n",
      "                Ticket_ca 2144        1      True\n",
      "             Ticket_c.a. 37671        1      True\n",
      "               Ticket_ca. 2314        1      True\n",
      "                 Ticket_349237        1      True\n",
      "                 Ticket_343120        1      True\n",
      "                   Ticket_2663        1      True\n",
      "                   Ticket_2661        1      True\n",
      "                 Ticket_345764        1      True\n",
      "                   Ticket_2653        1      True\n",
      "                   Ticket_2651        1      True\n",
      "              Ticket_c.a. 2673        1      True\n",
      "                   Ticket_2650        1      True\n",
      "              Ticket_c.a. 2315        1      True\n",
      "                 Ticket_c 7077        1      True\n",
      "                Ticket_c 17369        1      True\n",
      "               Ticket_ca. 2343        1      True\n",
      "       Ticket_ston/o2. 3101290        1      True\n",
      "              Ticket_sw/pp 751        1      True\n",
      "      Ticket_ston/o 2. 3101269        1      True\n",
      "      Ticket_ston/o 2. 3101285        1      True\n",
      "      Ticket_ston/o 2. 3101286        1      True\n",
      "          Pclass Ticket_111369        1      True\n",
      "          Pclass Ticket_111426        1      True\n",
      "               Age Age_missing        1      True\n",
      "       Ticket_ston/o2. 3101271        1      True\n",
      "                     Age pca_1        1      True\n",
      "                   Age cluster        1      True\n",
      "                      Sex_male        1      True\n",
      "                 Ticket_345572        1      True\n",
      "             Ticket_a/5. 10482        1      True\n",
      "               Ticket_a/5 3540        1      True\n",
      "                   Ticket_2665        1      True\n",
      "                 Ticket_248738        1      True\n",
      "                 Ticket_111427        1      True\n",
      "                 Ticket_111428        1      True\n",
      "              Ticket_a/5. 3336        1      True\n",
      "                   Ticket_7552        1      True\n",
      "                   Ticket_7553        1      True\n",
      "                   Ticket_7598        1      True\n",
      "                 Ticket_349236        1      True\n",
      "                 Ticket_248747        1      True\n",
      "                 Ticket_112277        1      True\n",
      "                 Ticket_113055        1      True\n",
      "                 Ticket_113059        1      True\n",
      "                 Ticket_113051        1      True\n",
      "                   Ticket_2627        1      True\n",
      "                   Ticket_2620        1      True\n",
      "                 Ticket_250651        1      True\n",
      "                 Ticket_345774        1      True\n",
      "                 Ticket_113794        1      True\n",
      "                 Ticket_248698        1      True\n",
      "                   Ticket_4136        1      True\n",
      "                   Ticket_4138        1      True\n",
      "                   Ticket_4137        1      True\n",
      "                 Ticket_113503        1      True\n",
      "                 Ticket_113501        1      True\n",
      "                 Ticket_113781        1      True\n",
      "                 Ticket_113786        1      True\n",
      "                 Ticket_113760        1      True\n",
      "                 Ticket_113773        1      True\n",
      "                 Ticket_113788        1      True\n",
      "                   Ticket_4134        1      True\n",
      "                  Ticket_13214        1      True\n",
      "                  Ticket_13213        1      True\n",
      "                 Ticket_113804        1      True\n",
      "                 Ticket_113806        1      True\n",
      "                 Ticket_345773        1      True\n",
      "                  Ticket_65306        1      True\n",
      "                 Ticket_244373        1      True\n",
      "                         pca_2        1      True\n",
      "                         pca_1        1      True\n",
      "                          Fare        1      True\n",
      "                           Age        1      True\n",
      "                 Ticket_363291        1      True\n",
      "                  Ticket_11967        1      True\n",
      "                  Ticket_11774        1      True\n",
      "                  Ticket_11765        1      True\n",
      "                  Ticket_11753        1      True\n",
      "                  Ticket_11751        1      True\n",
      "                  Ticket_11668        1      True\n",
      "      Ticket_ston/o 2. 3101288        1      True\n",
      "        Ticket_soton/oq 392089        1      True\n",
      "       Ticket_ston/o2. 3101283        1      True\n",
      "             Age Ticket_110413        1      True\n",
      "       Ticket_ston/o2. 3101279        1      True\n",
      "      Ticket_ston/o 2. 3101289        1      True\n",
      "       Ticket_ston/o2. 3101282        1      True\n",
      "          Pclass Ticket_110564        1      True\n",
      "                 Ticket_315096        1      True\n",
      "                 Ticket_315098        1      True\n",
      "          Pclass Ticket_110465        1      True\n",
      "                 Ticket_350407        2     False\n",
      "             Ticket_w./c. 6607        3     False\n",
      "              Ticket_a/5. 3337        4     False\n",
      "             Ticket_w./c. 6609        5     False\n",
      "      Ticket_soton/o.q. 392087        6     False\n",
      "                 Ticket_343095        7     False\n",
      "                   Ticket_2649        8     False\n",
      "                   Ticket_2626        9     False\n",
      "                    Embarked_s       10     False\n",
      "              cluster Sex_male       11     False\n",
      "            Fare Ticket_110465       12     False\n",
      "               Ticket_pc 17590       13     False\n",
      "                 Ticket_111426       14     False\n",
      "             Ticket_f.c. 12750       15     False\n",
      "                   Ticket_2695       16     False\n",
      "                 Ticket_347073       17     False\n",
      "                   Ticket_4135       18     False\n",
      "                   Ticket_7546       19     False\n",
      "             Age Ticket_111240       20     False\n",
      "          Pclass Ticket_111320       21     False\n",
      "                 Ticket_250649       22     False\n",
      "                 Ticket_112379       23     False\n",
      "                 Ticket_345763       24     False\n",
      "                  Ticket_13049       25     False\n",
      "                   Ticket_2690       26     False\n",
      "                 Ticket_113776       27     False\n",
      "                 Ticket_367230       28     False\n",
      "      Embarked_s Ticket_110564       29     False\n",
      "                 Ticket_113803       30     False\n",
      "               Ticket_pc 17600       31     False\n",
      "                 Ticket_231945       32     False\n",
      "                  Ticket_29108       33     False\n",
      "                 Ticket_236171       34     False\n",
      "                  Ticket_17463       35     False\n",
      "            Ticket_w.e.p. 5734       36     False\n",
      "          Ticket_sc/paris 2133       37     False\n",
      "                   Ticket_7534       38     False\n",
      "              Ticket_we/p 5735       39     False\n",
      "               Ticket_pc 17605       40     False\n",
      "               Ticket_pc 17612       41     False\n",
      "                 Ticket_113789       42     False\n",
      "                  Ticket_28665       43     False\n",
      "               Ticket_pc 17596       44     False\n",
      "                  Ticket_28424       45     False\n",
      "                  Ticket_19877       46     False\n",
      "               Ticket_pc 17757       47     False\n",
      "                 Ticket_113784       48     False\n",
      "           pca_1 Ticket_110465       49     False\n",
      "                  Ticket_13507       50     False\n",
      "                 Ticket_111369       51     False\n",
      "                 Ticket_230433       52     False\n",
      "             Ticket_c.a. 33112       53     False\n",
      "                 Ticket_110564       54     False\n",
      "                  Ticket_36973       55     False\n",
      "             Age Ticket_111320       56     False\n",
      "                  Ticket_36967       57     False\n",
      "                    Ticket_695       58     False\n",
      "           pca_1 Ticket_111426       59     False\n",
      "               Ticket_pc 17582       60     False\n",
      "           Sex_male Embarked_q       61     False\n",
      "               Ticket_pc 17597       62     False\n",
      "                 Ticket_113043       63     False\n",
      "                Ticket_3101298       64     False\n",
      "                 Ticket_113509       65     False\n",
      "                 Ticket_113050       66     False\n",
      "        Sex_male Ticket_111369       67     False\n",
      "                 Ticket_113796       68     False\n",
      "                 Ticket_113767       69     False\n",
      "             Pclass Embarked_s       70     False\n",
      "                 Ticket_113510       71     False\n",
      "           pca_2 Ticket_111426       72     False\n",
      "               Ticket_pc 17761       73     False\n",
      "                  Ticket_28134       74     False\n",
      "                  Ticket_36963       75     False\n",
      "               Ticket_pc 17483       76     False\n",
      "                   Ticket_2667       77     False\n",
      "                 Ticket_113028       78     False\n",
      "                 Ticket_113056       79     False\n",
      "               Ticket_pc 17318       80     False\n",
      "                   Ticket_5727       81     False\n",
      "                 Ticket_113787       82     False\n",
      "                   Ticket_2662       83     False\n",
      "                  Fare cluster       84     False\n",
      "                Pclass cluster       85     False\n",
      "                 Ticket_113798       86     False\n",
      "                 Ticket_231919       87     False\n",
      "                  Ticket_13509       88     False\n",
      "                 Ticket_113514       89     False\n",
      "                 Ticket_113807       90     False\n",
      "                 Ticket_113800       91     False\n",
      "                 Ticket_113792       92     False\n",
      "          Pclass Ticket_111240       93     False\n",
      "                    Ticket_693       94     False\n",
      "                   Ticket_2687       95     False\n",
      "          Ticket_sc/paris 2167       96     False\n",
      "                  Ticket_35273       97     False\n",
      "        Sex_male Ticket_110564       98     False\n",
      "                  Ticket_33638       99     False\n",
      "                 Ticket_370373      100     False\n",
      "               Ticket_pc 17609      101     False\n",
      "           pca_1 Ticket_111320      102     False\n",
      "                   Ticket_2680      103     False\n",
      "                  Ticket_28425      104     False\n",
      "                 Ticket_233639      105     False\n",
      "                  Ticket_29751      106     False\n",
      "                 Ticket_110465      107     False\n",
      "                  Ticket_29104      108     False\n",
      "                 Ticket_244278      109     False\n",
      "                   Ticket_2669      110     False\n",
      "                 Ticket_330958      111     False\n",
      "                   Ticket_2648      112     False\n",
      "               Ticket_pc 17601      113     False\n",
      "                   Ticket_2693      114     False\n",
      "                 Ticket_367231      115     False\n",
      "                  Ticket_35851      116     False\n",
      "                 Ticket_330923      117     False\n",
      "                  Pclass pca_2      118     False\n",
      "           Age_missing cluster      119     False\n",
      "        Sex_male Ticket_110465      120     False\n",
      "                  Ticket_54636      121     False\n",
      "                  Ticket_28403      122     False\n",
      "                 Ticket_250644      123     False\n",
      "           pca_1 Ticket_110413      124     False\n",
      "                 Ticket_233866      125     False\n",
      "                   Ticket_2625      126     False\n",
      "             Ticket_c.a. 29566      127     False\n",
      "                  Ticket_26360      128     False\n",
      "                 Ticket_358585      129     False\n",
      "      Embarked_s Ticket_110465      130     False\n",
      "                   Ticket_2672      131     False\n",
      "           pca_1 Ticket_110564      132     False\n",
      "                 Ticket_234686      133     False\n",
      "                 Ticket_244361      134     False\n",
      "             Ticket_c.a. 29178      135     False\n",
      "                 Ticket_236853      136     False\n",
      "                  Ticket_28220      137     False\n",
      "           pca_1 Ticket_111240      138     False\n",
      "                 Ticket_250647      139     False\n",
      "                  Ticket_35852      140     False\n",
      "                  Ticket_36866      141     False\n",
      "                 Ticket_370370      142     False\n",
      "                 Ticket_335677      143     False\n",
      "                  Ticket_14313      144     False\n",
      "                  Ticket_14311      145     False\n",
      "                  Ticket_14312      146     False\n",
      "                 Ticket_370375      147     False\n",
      "                   Ticket_9234      148     False\n",
      "                 Ticket_330932      149     False\n",
      "                 Ticket_330919      150     False\n",
      "                 Ticket_330931      151     False\n",
      "                 Ticket_330980      152     False\n",
      "                 Ticket_330959      153     False\n",
      "              Ticket_a/4 48871      154     False\n",
      "           pca_1 Ticket_111369      155     False\n",
      "                  Ticket_31028      156     False\n",
      "                 Ticket_347074      157     False\n",
      "             Ticket_a/4. 20589      158     False\n",
      "                 Ticket_345778      159     False\n",
      "                 Ticket_244358      160     False\n",
      "                 Ticket_211536      161     False\n",
      "                 Ticket_220367      162     False\n",
      "           pca_2 Ticket_111369      163     False\n",
      "        Sex_male Ticket_110413      164     False\n",
      "                 Ticket_315086      165     False\n",
      "                 Ticket_315093      166     False\n",
      "                 Ticket_315090      167     False\n",
      "      Ticket_ston/o 2. 3101274      168     False\n",
      "                 Ticket_350048      169     False\n",
      "                 Ticket_226875      170     False\n",
      "                 Ticket_218629      171     False\n",
      "                 Ticket_248740      172     False\n",
      "             Ticket_sc/ah 3085      173     False\n",
      "               Ticket_pc 17754      174     False\n",
      "                 Ticket_237789      175     False\n",
      "       Ticket_c.a./soton 34068      176     False\n",
      "            Ticket_w./c. 14258      177     False\n",
      "           Ticket_f.c.c. 13531      178     False\n",
      "                   Ticket_2223      179     False\n",
      "                 Ticket_350404      180     False\n",
      "                 Ticket_350036      181     False\n",
      "                 Ticket_349912      182     False\n",
      "                 Ticket_347078      183     False\n",
      "                Ticket_3101267      184     False\n",
      "                   Ticket_2697      185     False\n",
      "                   Ticket_2926      186     False\n",
      "                 Ticket_228414      187     False\n",
      "                 Ticket_223596      188     False\n",
      "                 Ticket_236852      189     False\n",
      "                   Ticket_2003      190     False\n",
      "           Ticket_f.c.c. 13528      191     False\n",
      "     Age_missing Ticket_111426      192     False\n",
      "             Ticket_c.a. 33595      193     False\n",
      "                 Ticket_111320      194     False\n",
      "            Ticket_w./c. 14263      195     False\n",
      "                 Ticket_349228      196     False\n",
      "              Ticket_s.p. 3464      197     False\n",
      "                 Ticket_323951      198     False\n",
      "                 Ticket_349231      199     False\n",
      "                 Ticket_349212      200     False\n",
      "                 Ticket_349205      201     False\n",
      "                 Ticket_347069      202     False\n",
      "                 Ticket_348124      203     False\n",
      "                 Ticket_250653      204     False\n",
      "                 Ticket_250646      205     False\n",
      "                  Ticket_28228      206     False\n",
      "                 Ticket_345769      207     False\n",
      "                   Ticket_6563      208     False\n",
      "                 Ticket_349256      209     False\n",
      "                  Age Sex_male      210     False\n",
      "                 Ticket_315094      211     False\n",
      "              Ticket_a/5. 2151      212     False\n",
      "       Ticket_soton/o2 3101287      213     False\n",
      "                 Ticket_350050      214     False\n",
      "                 Ticket_347466      215     False\n",
      "                  Ticket_31418      216     False\n",
      "     Ticket_soton/o.q. 3101307      217     False\n",
      "                   Ticket_2685      218     False\n",
      "                   Ticket_2694      219     False\n",
      "          Ticket_sc/paris 2149      220     False\n",
      "              Ticket_w/c 14208      221     False\n",
      "      Embarked_s Ticket_111320      222     False\n",
      "                  Ticket_17764      223     False\n",
      "              Ticket_a/5 21173      224     False\n",
      "             Age Ticket_111426      225     False\n",
      "                  Ticket_28551      226     False\n",
      "                  Ticket_27849      227     False\n",
      "                 Ticket_315097      228     False\n",
      "                   Ticket_8475      229     False\n",
      "              Ticket_a/4 45380      230     False\n",
      "             Ticket_a./5. 2152      231     False\n",
      "      Ticket_ston/o 2. 3101294      232     False\n",
      "      Ticket_ston/o 2. 3101280      233     False\n",
      "                 Ticket_350029      234     False\n",
      "             Ticket_a/4. 39886      235     False\n",
      "                 Ticket_312992      236     False\n",
      "       Ticket_soton/oq 3101317      237     False\n",
      "                  Ticket_11771      238     False\n",
      "             Ticket_c.a. 18723      239     False\n",
      "                 Ticket_243880      240     False\n",
      "        Sex_male Ticket_111320      241     False\n",
      "                  Ticket_28664      242     False\n",
      "            Fare Ticket_110413      243     False\n",
      "             Ticket_c.a. 34260      244     False\n",
      "     Age_missing Ticket_110465      245     False\n",
      "             Age Ticket_110465      246     False\n",
      "                 Ticket_349239      247     False\n",
      "                  Ticket_27267      248     False\n",
      "                Ticket_pp 4348      249     False\n",
      "                 Ticket_345767      250     False\n",
      "                 Ticket_234604      251     False\n",
      "                 Ticket_324669      252     False\n",
      "                  Ticket_14973      253     False\n",
      "                 Ticket_349206      254     False\n",
      "                 Ticket_349252      255     False\n",
      "                 Ticket_350052      256     False\n",
      "                 Ticket_350060      257     False\n",
      "              Ticket_a/5 21171      258     False\n",
      "              Ticket_a/5 21174      259     False\n",
      "              Ticket_a/5 21172      260     False\n",
      "      Ticket_ston/o 2. 3101275      261     False\n",
      "             Ticket_c.a. 33111      262     False\n",
      "             Ticket_sco/w 1585      263     False\n",
      "                 Ticket_248706      264     False\n",
      "                   Ticket_7267      265     False\n",
      "            Ticket_sc/ah 29037      266     False\n",
      "                   Ticket_3411      267     False\n",
      "                 Ticket_349204      268     False\n",
      "                 Ticket_347468      269     False\n",
      "                  Ticket_12233      270     False\n",
      "                 Ticket_248727      271     False\n",
      "                  Ticket_29105      272     False\n",
      "                 Ticket_237736      273     False\n",
      "          Ticket_sc/paris 2131      274     False\n",
      "                      Age Fare      275     False\n",
      "                 Ticket_349253      276     False\n",
      "                 Ticket_349254      277     False\n",
      "                 Ticket_234818      278     False\n",
      "              pca_2 Embarked_s      279     False\n",
      "            Pclass Age_missing      280     False\n",
      "          Age_missing Sex_male      281     False\n",
      "                   Age_missing      282     False\n",
      "             Age_missing pca_2      283     False\n",
      "                   pca_1 pca_2      284     False\n",
      "                    Fare pca_2      285     False\n",
      "                 Ticket_111240      286     False\n",
      "                 Ticket_376566      287     False\n",
      "                 Ticket_250648      288     False\n",
      "                 Ticket_349241      289     False\n",
      "                   Ticket_2629      290     False\n",
      "                   Ticket_2671      291     False\n",
      "                   Ticket_2641      292     False\n",
      "                   Ticket_2700      293     False\n",
      "                   Ticket_2686      294     False\n",
      "                   Ticket_2647      295     False\n",
      "                   Ticket_2624      296     False\n",
      "                   Ticket_2631      297     False\n",
      "                   Ticket_2664      298     False\n",
      "                   Ticket_2674      299     False\n",
      "                 Ticket_345781      300     False\n",
      "                 Ticket_343275      301     False\n",
      "                 Ticket_349233      302     False\n",
      "                 Ticket_350035      303     False\n",
      "                 Ticket_349209      304     False\n",
      "     Ticket_soton/o.q. 3101311      305     False\n",
      "        Sex_male Ticket_111240      306     False\n",
      "                 Ticket_342826      307     False\n",
      "                 Ticket_230434      308     False\n",
      "                 Ticket_237668      309     False\n",
      "             Ticket_c.a. 29395      310     False\n",
      "                 Ticket_370371      311     False\n",
      "                  Ticket_36568      312     False\n",
      "                 Ticket_240929      313     False\n",
      "            Fare Ticket_111320      314     False\n",
      "                 Ticket_229236      315     False\n",
      "                 Ticket_349203      316     False\n",
      "                 Ticket_347076      317     False\n",
      "                 Ticket_348123      318     False\n",
      "      Embarked_s Ticket_111240      319     False\n",
      "                 Ticket_374887      320     False\n",
      "                  Ticket_28206      321     False\n",
      "     Ticket_soton/o.q. 3101312      322     False\n",
      "        Ticket_soton/oq 392076      323     False\n",
      "                   Ticket_2699      324     False\n",
      "                  Ticket_26707      325     False\n",
      "            Fare Ticket_111369      326     False\n",
      "                 Ticket_349213      327     False\n",
      "            cluster Embarked_s      328     False\n",
      "          Ticket_sc/paris 2163      329     False\n",
      "                 Ticket_315151      330     False\n",
      "                 Ticket_341826      331     False\n",
      "     Age_missing Ticket_111369      332     False\n",
      "                 Ticket_349224      333     False\n",
      "                 Ticket_349248      334     False\n",
      "                   Ticket_3474      335     False\n",
      "                 Ticket_350025      336     False\n",
      "                 Ticket_347068      337     False\n",
      "                  Ticket_34218      338     False\n",
      "             Ticket_c.a. 17248      339     False\n",
      "        Ticket_sc/ah basle 541      340     False\n",
      "           Ticket_f.c.c. 13529      341     False\n",
      "                 Ticket_c 4001      342     False\n",
      "     Age_missing Ticket_110564      343     False\n",
      "                 Ticket_349219      344     False\n",
      "                   Ticket_2683      345     False\n",
      "                 Ticket_248733      346     False\n",
      "          Pclass Ticket_110413      347     False\n",
      "                 Ticket_345783      348     False\n",
      "                 Ticket_345770      349     False\n",
      "                 Ticket_248723      350     False\n",
      "                  Ticket_28213      351     False\n",
      "            Fare Ticket_111426      352     False\n",
      "                Ticket_3101277      353     False\n",
      "                 Ticket_349243      354     False\n",
      "                 Ticket_349207      355     False\n",
      "                 Ticket_347464      356     False\n",
      "                 Ticket_350042      357     False\n",
      "             Pclass Embarked_q      358     False\n",
      "                   Ticket_7545      359     False\n",
      "               Fare Embarked_q      360     False\n",
      "              pca_2 Embarked_q      361     False\n",
      "                 Ticket_343276      362     False\n",
      "                 Ticket_315082      363     False\n",
      "                 Ticket_347067      364     False\n",
      "             Ticket_c.a. 34651      365     False\n",
      "     Embarked_missing Sex_male      366     False\n",
      "                   Ticket_3460      367     False\n",
      "         cluster Ticket_111426      368     False\n",
      "             Ticket_a.5. 18509      369     False\n",
      "        Ticket_soton/oq 392090      370     False\n",
      "                 Ticket_244310      371     False\n",
      "                 Ticket_349246      372     False\n",
      "                 Ticket_330877      373     False\n",
      "                 Ticket_330979      374     False\n",
      "                 Ticket_368703      375     False\n",
      "                 Ticket_370372      376     False\n",
      "                 Ticket_372622      377     False\n",
      "                 Ticket_383121      378     False\n",
      "                 Ticket_370377      379     False\n",
      "                 Ticket_384461      380     False\n",
      "                  Ticket_12460      381     False\n",
      "                 Ticket_367229      382     False\n",
      "                 Ticket_364851      383     False\n",
      "                 Ticket_371060      384     False\n",
      "                  Ticket_36865      385     False\n",
      "                 Ticket_334912      386     False\n",
      "                 Ticket_367655      387     False\n",
      "                  Ticket_36209      388     False\n",
      "                 Ticket_c 7076      389     False\n",
      "                 Ticket_364499      390     False\n",
      "        Age_missing Embarked_s      391     False\n",
      "          Ticket_sc/paris 2123      392     False\n",
      "                 Ticket_347063      393     False\n",
      "                 Ticket_368323      394     False\n",
      "                 Ticket_394140      395     False\n",
      "             Ticket_c.a. 31026      396     False\n",
      "                  Ticket_65303      397     False\n",
      "                  Ticket_65304      398     False\n",
      "                 Ticket_349910      399     False\n",
      "                   Ticket_8471      400     False\n",
      "                 Ticket_237565      401     False\n",
      "                 Ticket_345780      402     False\n",
      "      Ticket_ston/o 2. 3101293      403     False\n",
      "      Ticket_ston/o 2. 3101292      404     False\n",
      "                 Ticket_349242      405     False\n",
      "                   Ticket_2623      406     False\n",
      "                   Ticket_7540      407     False\n",
      "                 Ticket_371110      408     False\n",
      "              Ticket_p/pp 3381      409     False\n",
      "                 Ticket_349257      410     False\n",
      "                 Ticket_347062      411     False\n",
      "                 Ticket_250652      412     False\n",
      "           pca_2 Ticket_110564      413     False\n",
      "              Ticket_c.a. 6212      414     False\n",
      "                 Ticket_364498      415     False\n",
      "                 Ticket_363294      416     False\n",
      "                 Ticket_364506      417     False\n",
      "                Ticket_sc 1748      418     False\n",
      "            Fare Ticket_111240      419     False\n",
      "               Ticket_a/5. 851      420     False\n",
      "                 Ticket_234360      421     False\n",
      "                 Ticket_371362      422     False\n",
      "                       cluster      423     False\n",
      "                 pca_2 cluster      424     False\n",
      "                 Ticket_350026      425     False\n",
      "                  Ticket_24160      426     False\n",
      "                 Ticket_364512      427     False\n",
      "                 Ticket_373450      428     False\n",
      "                Ticket_3101264      429     False\n",
      "                  Ticket_19972      430     False\n",
      "            Fare Ticket_110564      431     False\n",
      "                 Ticket_250643      432     False\n",
      "      Ticket_ston/o 2. 3101273      433     False\n",
      "     Ticket_soton/o.q. 3101310      434     False\n",
      "                 Ticket_349247      435     False\n",
      "                  Ticket_29750      436     False\n",
      "         cluster Ticket_111369      437     False\n",
      "             Ticket_c.a. 31921      438     False\n",
      "                 Ticket_248731      439     False\n",
      "                 Ticket_349210      440     False\n",
      "                  Ticket_29011      441     False\n",
      "             Ticket_so/c 14885      442     False\n",
      "     Age_missing Ticket_111320      443     False\n",
      "                Ticket_3101276      444     False\n",
      "            Ticket_s.o.p. 1166      445     False\n",
      "                 Ticket_315089      446     False\n",
      "                 Ticket_237442      447     False\n",
      "                 Ticket_365222      448     False\n",
      "                 Ticket_349249      449     False\n",
      "                Age Embarked_s      450     False\n",
      "                     Age pca_2      451     False\n",
      "         cluster Ticket_110564      452     False\n",
      "                 Ticket_345777      453     False\n",
      "                 Ticket_112050      454     False\n",
      "             Age Ticket_111369      455     False\n",
      "                   Ticket_2628      456     False\n",
      "                Ticket_3101296      457     False\n",
      "      Embarked_s Ticket_110413      458     False\n",
      "     Ticket_soton/o.q. 3101306      459     False\n",
      "                 Ticket_315037      460     False\n",
      "            Fare Ticket_111361      461     False\n",
      "                 Ticket_349251      462     False\n",
      "                 Ticket_392092      463     False\n",
      "               Ticket_a/s 2816      464     False\n",
      "              Ticket_a4. 54510      465     False\n",
      "                 Ticket_374910      466     False\n",
      "        Ticket_s.c./a.4. 23567      467     False\n",
      "               Ticket_a/5 2817      468     False\n",
      "                 Ticket_374746      469     False\n",
      "       Ticket_soton/oq 3101316      470     False\n",
      "             Ticket_a./5. 3235      471     False\n",
      "        Ticket_soton/oq 392086      472     False\n",
      "        Ticket_soton/oq 392082      473     False\n",
      "               Ticket_a/5 2466      474     False\n",
      "                 Ticket_349218      475     False\n",
      "                 Ticket_349214      476     False\n",
      "                 Ticket_349216      477     False\n",
      "                 Ticket_349208      478     False\n",
      "                 Ticket_349222      479     False\n",
      "                 Ticket_349215      480     False\n",
      "                 Ticket_349217      481     False\n",
      "                 Ticket_349221      482     False\n",
      "                 Ticket_349234      483     False\n",
      "                 Ticket_349227      484     False\n",
      "                 Ticket_349223      485     False\n",
      "                 Ticket_349225      486     False\n",
      "                 Ticket_349201      487     False\n",
      "                 Ticket_312993      488     False\n",
      "                 Ticket_315088      489     False\n",
      "                 Ticket_110413      490     False\n",
      "          Ticket_s.o./p.p. 751      491     False\n",
      "         cluster Ticket_110465      492     False\n",
      "                   Ticket_4579      493     False\n",
      "              Ticket_fa 265302      494     False\n",
      "                 Ticket_362316      495     False\n",
      "                 Ticket_323592      496     False\n",
      "             Ticket_a/4. 34244      497     False\n",
      "     Ticket_soton/o.q. 3101305      498     False\n",
      "                  Ticket_13502      499     False\n",
      "     Age_missing Ticket_111240      500     False\n",
      "                  Ticket_16966      501     False\n",
      "                  Ticket_29103      502     False\n",
      "                 Ticket_112059      503     False\n",
      "                 Ticket_370365      504     False\n",
      "       Ticket_soton/o2 3101272      505     False\n",
      "                 Ticket_348121      506     False\n",
      "          Pclass Ticket_111361      507     False\n",
      "               Ticket_a/5 3536      508     False\n",
      "              Ticket_c.a. 5547      509     False\n",
      "             Ticket_c.a. 24579      510     False\n",
      "             Ticket_c.a. 24580      511     False\n",
      "                pca_1 Sex_male      512     False\n",
      "                 Ticket_364511      513     False\n",
      "                 Ticket_363592      514     False\n",
      "                 Ticket_345765      515     False\n",
      "             Ticket_a/5. 13032      516     False\n",
      "              pca_1 Embarked_q      517     False\n",
      "             Age_missing pca_1      518     False\n",
      "              Fare Age_missing      519     False\n",
      "           pca_2 Ticket_110465      520     False\n",
      "                   Ticket_2659      521     False\n",
      "           pca_1 Ticket_111361      522     False\n",
      "                 Ticket_c 7075      523     False\n",
      "                  Ticket_36928      524     False\n",
      "                 Ticket_347061      525     False\n",
      "                 Ticket_350047      526     False\n",
      "               Ticket_pc 17477      527     False\n",
      "               Ticket_a/5 3594      528     False\n",
      "               Ticket_a/5 3902      529     False\n",
      "           pca_2 Ticket_111361      530     False\n",
      "               Ticket_pc 17569      531     False\n",
      "                  Ticket_21440      532     False\n",
      "                 Ticket_113505      533     False\n",
      "                 Ticket_347064      534     False\n",
      "                  Ticket_11767      535     False\n",
      "                  Ticket_36947      536     False\n",
      "             Ticket_a.5. 11206      537     False\n",
      "                 Ticket_239853      538     False\n",
      "                  Ticket_12749      539     False\n",
      "                 Ticket_347060      540     False\n",
      "             Age Ticket_110813      541     False\n",
      "                 Ticket_347743      542     False\n",
      "                  Ticket_17421      543     False\n",
      "                 Ticket_364500      544     False\n",
      "               Ticket_pc 17608      545     False\n",
      "         cluster Ticket_111320      546     False\n",
      "                  Ticket_19928      547     False\n",
      "        Ticket_s.c./paris 2079      548     False\n",
      "                 Ticket_112058      549     False\n",
      "                 Ticket_112052      550     False\n",
      "          Age Embarked_missing      551     False\n",
      "                 Ticket_345364      552     False\n",
      "           pca_1 Ticket_110813      553     False\n",
      "                 Ticket_226593      554     False\n",
      "        Age_missing Embarked_q      555     False\n",
      "         cluster Ticket_111240      556     False\n",
      "                  Ticket_36864      557     False\n",
      "            Fare Ticket_110813      558     False\n",
      "                 Ticket_243847      559     False\n",
      "           pca_2 Ticket_110813      560     False\n",
      "                 Ticket_111361      561     False\n",
      "        Embarked_missing pca_2      562     False\n",
      "               Ticket_pc 17599      563     False\n",
      "               Ticket_pc 17756      564     False\n",
      "                  Ticket_11813      565     False\n",
      "               Ticket_pc 17610      566     False\n",
      "     Age_missing Ticket_110413      567     False\n",
      "          Pclass Ticket_110813      568     False\n",
      "                  Ticket_17464      569     False\n",
      "                Age Embarked_q      570     False\n",
      "                  Ticket_11755      571     False\n",
      "               Ticket_pc 17585      572     False\n",
      "            cluster Embarked_q      573     False\n",
      "             Age Ticket_110564      574     False\n",
      "               Ticket_pc 17603      575     False\n",
      "       Pclass Embarked_missing      576     False\n",
      "         Fare Embarked_missing      577     False\n",
      "                 Ticket_335097      578     False\n",
      "                 Ticket_370376      579     False\n",
      "         cluster Ticket_110413      580     False\n",
      "                 Ticket_110813      581     False\n",
      "               Ticket_pc 17482      582     False\n",
      "     Age_missing Ticket_111361      583     False\n",
      "                 Ticket_239855      584     False\n",
      "                 Ticket_239856      585     False\n",
      "                 Ticket_239854      586     False\n",
      "                  Ticket_13568      587     False\n",
      "Embarked_missing Ticket_111426      588     False\n",
      "                  Ticket_11769      589     False\n",
      "           pca_2 Ticket_110413      590     False\n",
      "              Embarked_missing      591     False\n",
      "             Age Ticket_111361      592     False\n",
      "                   Pclass Fare      593     False\n",
      "                  Ticket_17466      594     False\n",
      "                  Ticket_17465      595     False\n",
      "                 Ticket_113783      596     False\n",
      "Embarked_missing Ticket_111369      597     False\n",
      "Embarked_missing Ticket_110564      598     False\n",
      "           pca_2 Ticket_111320      599     False\n",
      "         cluster Ticket_111361      600     False\n",
      "   Embarked_missing Embarked_s      601     False\n",
      "                 Ticket_367232      602     False\n",
      "Embarked_missing Ticket_110465      603     False\n",
      "     Age_missing Ticket_110813      604     False\n",
      "               Ticket_pc 17592      605     False\n",
      "                    Embarked_q      606     False\n",
      "                 pca_1 cluster      607     False\n",
      "Embarked_missing Ticket_111320      608     False\n",
      "                 Ticket_219533      609     False\n",
      "                 Ticket_112053      610     False\n",
      "Embarked_missing Ticket_111240      611     False\n",
      "         cluster Ticket_110813      612     False\n",
      "           pca_2 Ticket_111240      613     False\n",
      "                  Ticket_11752      614     False\n",
      "      Embarked_missing cluster      615     False\n",
      "                 Ticket_370369      616     False\n",
      "                 Ticket_336439      617     False\n",
      "        Embarked_missing pca_1      618     False\n",
      "                 Ticket_250655      619     False\n",
      "Embarked_missing Ticket_110413      620     False\n",
      "Embarked_missing Ticket_111361      621     False\n",
      "  Embarked_missing Age_missing      622     False\n",
      "                  Ticket_31027      623     False\n",
      "                 Ticket_244367      624     False\n",
      "Embarked_missing Ticket_110813      625     False\n",
      "   Embarked_missing Embarked_q      626     False\n",
      "                 Ticket_113572      627     False\n",
      "   Ticket_110564 Ticket_110813      628     False\n",
      "   Ticket_111369 Ticket_111426      629     False\n",
      "   Ticket_111240 Ticket_111361      630     False\n",
      "   Ticket_110413 Ticket_111369      631     False\n",
      "   Ticket_110564 Ticket_111426      632     False\n",
      "   Ticket_110564 Ticket_111320      633     False\n",
      "   Ticket_110564 Ticket_111361      634     False\n",
      "   Ticket_110465 Ticket_111426      635     False\n",
      "   Ticket_111320 Ticket_111369      636     False\n",
      "   Ticket_110465 Ticket_110564      637     False\n",
      "   Ticket_110465 Ticket_111240      638     False\n",
      "   Ticket_111240 Ticket_111426      639     False\n",
      "   Ticket_110413 Ticket_110564      640     False\n",
      "      Embarked_q Ticket_110564      641     False\n",
      "   Ticket_110413 Ticket_111426      642     False\n",
      "   Ticket_110813 Ticket_111240      643     False\n",
      "   Ticket_110413 Ticket_110465      644     False\n",
      "   Ticket_110813 Ticket_111426      645     False\n",
      "   Ticket_110413 Ticket_111320      646     False\n",
      "   Ticket_110465 Ticket_110813      647     False\n",
      "      Embarked_q Ticket_110465      648     False\n",
      "      Embarked_q Ticket_111361      649     False\n",
      "      Embarked_s Ticket_111369      650     False\n",
      "      Embarked_s Ticket_111426      651     False\n",
      "      Embarked_q Ticket_111240      652     False\n",
      "      Embarked_q Ticket_111320      653     False\n",
      "   Ticket_110413 Ticket_111361      654     False\n",
      "   Ticket_110813 Ticket_111361      655     False\n",
      "   Ticket_110413 Ticket_110813      656     False\n",
      "   Ticket_111240 Ticket_111369      657     False\n",
      "   Ticket_110564 Ticket_111240      658     False\n",
      "   Ticket_110813 Ticket_111369      659     False\n",
      "      Embarked_s Ticket_111361      660     False\n",
      "   Ticket_110465 Ticket_111369      661     False\n",
      "   Ticket_110813 Ticket_111320      662     False\n",
      "   Ticket_110465 Ticket_111320      663     False\n",
      "   Ticket_110413 Ticket_111240      664     False\n",
      "   Ticket_111361 Ticket_111369      665     False\n",
      "      Embarked_q Ticket_111369      666     False\n",
      "   Ticket_110564 Ticket_111369      667     False\n",
      "   Ticket_111320 Ticket_111426      668     False\n",
      "        Sex_male Ticket_111361      669     False\n",
      "   Ticket_110465 Ticket_111361      670     False\n",
      "   Ticket_111320 Ticket_111361      671     False\n",
      "      Embarked_s Ticket_110813      672     False\n",
      "      Embarked_q Ticket_111426      673     False\n",
      "   Ticket_111240 Ticket_111320      674     False\n",
      "      Embarked_q Ticket_110813      675     False\n",
      "      Embarked_q Ticket_110413      676     False\n",
      "        Sex_male Ticket_110813      677     False\n",
      "         Embarked_q Embarked_s      678     False\n",
      "   Ticket_111361 Ticket_111426      679     False\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import clone\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def step23_backward_feature_elimination(\n",
    "    X, y,\n",
    "    model=None,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    min_features_to_select=1,\n",
    "    use_interactions=False,\n",
    "    poly_degree=2,\n",
    "    max_interaction_features=20,       # ⛔ LIMIT feature count for interaction\n",
    "    max_poly_features=500,             # ⛔ HARD STOP if poly features too large\n",
    "    include_feature_ranking=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Step 23: Backward Feature Elimination with RFECV and optional interaction terms.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n🧪 Step 23: Backward Feature Elimination (RFECV)\")\n",
    "    print(\"📌 Starting with all features and evaluating removals...\")\n",
    "\n",
    "    if model is None:\n",
    "        model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "    # Separate numeric and categorical columns\n",
    "    num_cols = X.select_dtypes(include='number').columns.tolist()\n",
    "    cat_cols = X.select_dtypes(exclude='number').columns.tolist()\n",
    "\n",
    "    # Drop constant/all-NaN columns\n",
    "    X_num_raw = X[num_cols].copy()\n",
    "    valid_num_cols = [col for col in num_cols if X_num_raw[col].notna().sum() > 0 and X_num_raw[col].nunique() > 1]\n",
    "    dropped_cols = list(set(num_cols) - set(valid_num_cols))\n",
    "    if dropped_cols:\n",
    "        print(f\"⚠️ Dropped {len(dropped_cols)} constant/all-NaN numeric columns: {dropped_cols}\")\n",
    "\n",
    "    # Impute and scale\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    scaler = StandardScaler()\n",
    "    X_num = pd.DataFrame(imputer.fit_transform(X_num_raw[valid_num_cols]), columns=valid_num_cols)\n",
    "    X_num_scaled = pd.DataFrame(scaler.fit_transform(X_num), columns=valid_num_cols)\n",
    "\n",
    "    # Encode categorical\n",
    "    if cat_cols:\n",
    "        X_cat = pd.get_dummies(X[cat_cols], drop_first=True)\n",
    "        X_clean = pd.concat([X_num_scaled, X_cat], axis=1)\n",
    "    else:\n",
    "        X_clean = X_num_scaled.copy()\n",
    "\n",
    "    original_feature_names = X_clean.columns.tolist()\n",
    "\n",
    "    # Limit interaction features\n",
    "    if use_interactions:\n",
    "        limited_X = X_clean.iloc[:, :max_interaction_features]\n",
    "        print(f\"🔁 Generating interaction features (degree={poly_degree}) using first {limited_X.shape[1]} columns...\")\n",
    "        poly = PolynomialFeatures(degree=poly_degree, interaction_only=True, include_bias=False)\n",
    "        X_poly = poly.fit_transform(limited_X)\n",
    "\n",
    "        if X_poly.shape[1] > max_poly_features:\n",
    "            print(f\"❌ Skipping interaction: {X_poly.shape[1]} features exceeds limit of {max_poly_features}\")\n",
    "            X_final = X_clean.copy()\n",
    "            feature_names = np.array(original_feature_names)\n",
    "        else:\n",
    "            feature_names = poly.get_feature_names_out(limited_X.columns)\n",
    "            X_rest = X_clean.drop(columns=limited_X.columns)\n",
    "            X_final = pd.DataFrame(X_poly, columns=feature_names)\n",
    "            if not X_rest.empty:\n",
    "                X_final = pd.concat([X_final, X_rest.reset_index(drop=True)], axis=1)\n",
    "                feature_names = X_final.columns.tolist()\n",
    "    else:\n",
    "        X_final = X_clean.copy()\n",
    "        feature_names = np.array(original_feature_names)\n",
    "\n",
    "    print(f\"🔢 Total features considered: {len(feature_names)}\")\n",
    "\n",
    "    # RFECV\n",
    "    rfecv = RFECV(\n",
    "        estimator=clone(model),\n",
    "        step=1,\n",
    "        cv=StratifiedKFold(n_splits=cv),\n",
    "        scoring=scoring,\n",
    "        min_features_to_select=min_features_to_select,\n",
    "        n_jobs=1  # Use 1 core to reduce memory pressure\n",
    "    )\n",
    "\n",
    "    rfecv.fit(X_final, y)\n",
    "    selected_mask = rfecv.support_\n",
    "    selected_features = list(X_final.columns[selected_mask])\n",
    "\n",
    "    print(f\"\\n✅ Final Selected Features ({len(selected_features)}):\")\n",
    "    for feat in selected_features:\n",
    "        print(f\"   - {feat}\")\n",
    "\n",
    "    if include_feature_ranking:\n",
    "        print(\"\\n📊 Feature Ranking Summary:\")\n",
    "        ranking_df = pd.DataFrame({\n",
    "            'Feature': X_final.columns,\n",
    "            'Ranking': rfecv.ranking_,\n",
    "            'Selected': rfecv.support_\n",
    "        }).sort_values(by='Ranking')\n",
    "        print(ranking_df.to_string(index=False))\n",
    "        return selected_features, ranking_df\n",
    "\n",
    "    return selected_features, None\n",
    "selected_features, feature_ranking_df = step23_backward_feature_elimination(\n",
    "    X=df,\n",
    "    y=y,\n",
    "    model=LogisticRegression(solver='liblinear'),\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    min_features_to_select=5,\n",
    "    use_interactions=True,\n",
    "    poly_degree=2,\n",
    "    max_interaction_features=20,     # 🔐 Try increasing gradually if needed\n",
    "    max_poly_features=500,           # 🔐 Safety stop\n",
    "    include_feature_ranking=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "30ae55fa-ffdc-43b0-9118-49dbd0436b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked_missing</th>\n",
       "      <th>Age_missing</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_q</th>\n",
       "      <th>Embarked_s</th>\n",
       "      <th>pca_1</th>\n",
       "      <th>pca_2</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a/5 21171</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.692582</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.902406</td>\n",
       "      <td>1.452358</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pc 17599</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.101506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-2.377642</td>\n",
       "      <td>-2.039552</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ston/o2. 3101282</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.815138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.436028</td>\n",
       "      <td>0.807077</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>113803</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.286974</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>-2.377436</td>\n",
       "      <td>-0.488442</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>373450</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.837252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.605643</td>\n",
       "      <td>1.270767</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Ticket  Pclass   Age  Parch      Fare  Embarked_missing  \\\n",
       "0         a/5 21171       3  22.0    0.0  2.692582               0.0   \n",
       "1          pc 17599       1  38.0    0.0  8.101506               0.0   \n",
       "2  ston/o2. 3101282       3  26.0    0.0  2.815138               0.0   \n",
       "3            113803       1  35.0    0.0  7.286974               0.0   \n",
       "4            373450       3  35.0    0.0  2.837252               0.0   \n",
       "\n",
       "   Age_missing  Sex_male  Embarked_q  Embarked_s     pca_1     pca_2  cluster  \n",
       "0          0.0      True       False        True  0.902406  1.452358        0  \n",
       "1          0.0     False       False       False -2.377642 -2.039552        0  \n",
       "2          0.0     False       False        True  0.436028  0.807077        0  \n",
       "3          0.0     False       False        True -2.377436 -0.488442        0  \n",
       "4          0.0      True       False        True  0.605643  1.270767        0  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f9e3b56-a320-43ff-a52a-2709601fd8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔁 Running Stepwise Selection...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 115\u001b[39m\n\u001b[32m    110\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Particle Swarm Optimization selected features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[33m'\u001b[39m\u001b[33mpso\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m selected_features = \u001b[43mstep24_bi_directional_feature_selection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrandom_forest\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43maccuracy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcv_splits\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Optional: Limit to 10 features\u001b[39;49;00m\n\u001b[32m    122\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# View results for all methods\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m method, features \u001b[38;5;129;01min\u001b[39;00m selected_features.items():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mstep24_bi_directional_feature_selection\u001b[39m\u001b[34m(X, y, model_type, scoring, cv_splits, max_features, random_state)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m🔁 Running Stepwise Selection...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     53\u001b[39m sfs = SFS(estimator,\n\u001b[32m     54\u001b[39m           k_features=max_features \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mbest\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     55\u001b[39m           forward=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     58\u001b[39m           cv=cv_splits,\n\u001b[32m     59\u001b[39m           n_jobs=-\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m sfs = sfs.fit(X.values, \u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m)\n\u001b[32m     61\u001b[39m results[\u001b[33m'\u001b[39m\u001b[33mstepwise\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(X.columns[\u001b[38;5;28mlist\u001b[39m(sfs.k_feature_idx_)])\n\u001b[32m     62\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Stepwise selected features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[33m'\u001b[39m\u001b[33mstepwise\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "import optuna\n",
    "import pyswarms as ps\n",
    "\n",
    "\n",
    "def step24_bi_directional_feature_selection(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    model_type: str = 'logistic',\n",
    "    scoring: str = 'accuracy',\n",
    "    cv_splits: int = 5,\n",
    "    max_features: int = None,\n",
    "    random_state: int = 42\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Apply all bi-directional search methods for feature selection.\n",
    "\n",
    "    Parameters:\n",
    "        X (pd.DataFrame): Feature DataFrame\n",
    "        y (pd.Series): Target Series\n",
    "        model_type (str): 'logistic' or 'random_forest'\n",
    "        scoring (str): Scoring metric\n",
    "        cv_splits (int): Cross-validation splits\n",
    "        max_features (int): Optional max number of features\n",
    "        random_state (int): Random state for reproducibility\n",
    "\n",
    "    Returns:\n",
    "        dict: Selected features for each method\n",
    "    \"\"\"\n",
    "    if model_type == 'logistic':\n",
    "        estimator = LogisticRegression(max_iter=1000, solver='liblinear')\n",
    "    elif model_type == 'random_forest':\n",
    "        estimator = RandomForestClassifier(n_estimators=100, random_state=random_state)\n",
    "    else:\n",
    "        raise ValueError(\"❌ Invalid model_type. Choose 'logistic' or 'random_forest'.\")\n",
    "\n",
    "    def evaluate(features):\n",
    "        if not any(features): return 0\n",
    "        selected = X.loc[:, features]\n",
    "        score = cross_val_score(clone(estimator), selected, y, scoring=scoring, cv=cv_splits).mean()\n",
    "        return score\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # ---------- STEPWISE SELECTION ----------\n",
    "    print(\"🔁 Running Stepwise Selection...\")\n",
    "    sfs = SFS(estimator,\n",
    "              k_features=max_features or 'best',\n",
    "              forward=True,\n",
    "              floating=True,\n",
    "              scoring=scoring,\n",
    "              cv=cv_splits,\n",
    "              n_jobs=-1)\n",
    "    sfs = sfs.fit(X.values, y.values)\n",
    "    results['stepwise'] = list(X.columns[list(sfs.k_feature_idx_)])\n",
    "    print(f\"✅ Stepwise selected features: {results['stepwise']}\")\n",
    "\n",
    "    # ---------- GENETIC ALGORITHM ----------\n",
    "    print(\"🧬 Running Genetic Algorithm Optimization...\")\n",
    "    def genetic_objective(trial):\n",
    "        mask = [trial.suggest_categorical(f'f_{i}', [True, False]) for i in range(X.shape[1])]\n",
    "        if not any(mask): return 0\n",
    "        return evaluate(mask)\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(genetic_objective, n_trials=50, show_progress_bar=False)\n",
    "    best_mask = [study.best_trial.params.get(f'f_{i}', False) for i in range(X.shape[1])]\n",
    "    results['genetic'] = list(X.columns[np.array(best_mask)])\n",
    "    print(f\"✅ Genetic Algorithm selected features: {results['genetic']}\")\n",
    "\n",
    "    # ---------- SIMULATED ANNEALING ----------\n",
    "    print(\"🔥 Running Simulated Annealing...\")\n",
    "    from scipy.optimize import dual_annealing\n",
    "\n",
    "    def sa_objective(bitmask):\n",
    "        bitmask = np.round(bitmask).astype(bool)\n",
    "        return -evaluate(bitmask)  # negate because we minimize in SA\n",
    "\n",
    "    bounds = [(0, 1)] * X.shape[1]\n",
    "    sa_result = dual_annealing(sa_objective, bounds, seed=random_state)\n",
    "    sa_mask = np.round(sa_result.x).astype(bool)\n",
    "    results['annealing'] = list(X.columns[sa_mask])\n",
    "    print(f\"✅ Simulated Annealing selected features: {results['annealing']}\")\n",
    "\n",
    "    # ---------- PARTICLE SWARM OPTIMIZATION ----------\n",
    "    print(\"🐦 Running Particle Swarm Optimization...\")\n",
    "    dimensions = X.shape[1]\n",
    "\n",
    "    def pso_fitness(mask_array):\n",
    "        mask_array = np.round(mask_array).astype(bool)\n",
    "        scores = []\n",
    "        for row in mask_array:\n",
    "            if not any(row):\n",
    "                scores.append(0)\n",
    "            else:\n",
    "                scores.append(evaluate(row))\n",
    "        return -np.array(scores)\n",
    "\n",
    "    options = {'c1': 0.5, 'c2': 0.3, 'w': 0.9}\n",
    "    optimizer = ps.single.GlobalBestPSO(n_particles=10, dimensions=dimensions, options=options)\n",
    "    cost, pos = optimizer.optimize(pso_fitness, iters=30, verbose=False)\n",
    "    pso_mask = np.round(pos).astype(bool)\n",
    "    results['pso'] = list(X.columns[pso_mask])\n",
    "    print(f\"✅ Particle Swarm Optimization selected features: {results['pso']}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "selected_features = step24_bi_directional_feature_selection(\n",
    "    X=df,\n",
    "    y=y,\n",
    "    model_type='random_forest',\n",
    "    scoring='accuracy',\n",
    "    cv_splits=5,\n",
    "    max_features=10  # Optional: Limit to 10 features\n",
    ")\n",
    "\n",
    "# View results for all methods\n",
    "for method, features in selected_features.items():\n",
    "    print(f\"📊 {method.capitalize()} Selected Features: {features}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc18719-c8a4-4442-aaac-4203483ff869",
   "metadata": {},
   "source": [
    "Phase 3.3: Embedded Selection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5fb59ce-da60-413d-a501-33bcc5b516c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: group-lasso in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from group-lasso) (2.2.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from group-lasso) (1.7.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->group-lasso) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->group-lasso) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->group-lasso) (3.6.0)\n",
      "\n",
      "📦 Step 25: Regularization-Based Feature Selection\n",
      "\n",
      "🔹 Lasso selected 2 features:\n",
      "['Marks_zscore', 'cluster']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.00045175395145262567\n",
      "Best l1_ratio: 1.0\n",
      "Number of selected features: 2\n",
      "Coefficients: [-0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -0.00000000e+00  1.42936556e-04\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00 -4.51166560e-01]\n",
      "\n",
      "🔹 Elastic Net selected 2 features:\n",
      "['Marks_zscore', 'cluster']\n",
      "\n",
      "⚠️ Group Lasso failed: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "🔹 Adaptive Lasso selected 3 features:\n",
      "['Marks Attendance (%)', 'Marks Attendance (%)_zscore', 'pca_2']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install group-lasso\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LassoCV, ElasticNetCV, RidgeCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from group_lasso import GroupLasso\n",
    "from sklearn.base import clone\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def step25_regularization_feature_selection(X, y, group_map=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Applies multiple regularization-based feature selection methods:\n",
    "    - L1 (Lasso)\n",
    "    - Elastic Net\n",
    "    - Group Lasso\n",
    "    - Adaptive Lasso\n",
    "\n",
    "    Parameters:\n",
    "    - X (pd.DataFrame): Feature matrix\n",
    "    - y (pd.Series or array): Target variable\n",
    "    - group_map (list or np.array): Group mapping for Group Lasso\n",
    "    - verbose (bool): Whether to print selected features and scores\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary of selected features per method\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    feature_names = X.columns\n",
    "\n",
    "    print(\"\\n📦 Step 25: Regularization-Based Feature Selection\\n\")\n",
    "\n",
    "    # 1️⃣ Lasso (L1)\n",
    "    lasso = LassoCV(cv=5, random_state=42).fit(X_scaled, y)\n",
    "    selected_lasso = feature_names[lasso.coef_ != 0]\n",
    "    results[\"Lasso\"] = list(selected_lasso)\n",
    "    if verbose:\n",
    "        print(f\"🔹 Lasso selected {len(selected_lasso)} features:\\n{list(selected_lasso)}\")\n",
    "\n",
    "    # 2️⃣ ElasticNet (L1 + L2)\n",
    "    elastic = ElasticNetCV(cv=5, l1_ratio=[.1, .5, .7, .9, .95, .99, 1], random_state=42).fit(X_scaled, y)\n",
    "    selected_elastic = feature_names[elastic.coef_ != 0]\n",
    "    results[\"ElasticNet\"] = list(selected_elastic)\n",
    "    print(\"Best alpha:\", elastic.alpha_)\n",
    "    print(\"Best l1_ratio:\", elastic.l1_ratio_)\n",
    "    print(\"Number of selected features:\", sum(elastic.coef_ != 0))\n",
    "    print(\"Coefficients:\", elastic.coef_)\n",
    "    if verbose:\n",
    "        print(f\"\\n🔹 Elastic Net selected {len(selected_elastic)} features:\\n{list(selected_elastic)}\")\n",
    "\n",
    "    # 3️⃣ Group Lasso\n",
    "    if group_map is not None:\n",
    "        try:\n",
    "            gl = GroupLasso(groups=np.array(group_map), group_reg=0.05, l1_reg=0.01, scale_reg=\"group_size\", supress_warning=True)\n",
    "            gl.fit(X_scaled, y)\n",
    "            selected_group = feature_names[gl.coef_ != 0]\n",
    "            results[\"GroupLasso\"] = list(selected_group)\n",
    "            if verbose:\n",
    "                print(f\"\\n🔹 Group Lasso selected {len(selected_group)} features:\\n{list(selected_group)}\")\n",
    "        except Exception as e:\n",
    "            results[\"GroupLasso\"] = []\n",
    "            if verbose:\n",
    "                print(f\"\\n⚠️ Group Lasso failed: {e}\")\n",
    "    else:\n",
    "        results[\"GroupLasso\"] = []\n",
    "        if verbose:\n",
    "            print(\"\\n⚠️ Group Lasso skipped: No group_map provided.\")\n",
    "\n",
    "    # 4️⃣ Adaptive Lasso\n",
    "    try:\n",
    "        ridge = RidgeCV(cv=5).fit(X_scaled, y)\n",
    "        weights = 1 / (np.abs(ridge.coef_) + 1e-4)  # avoid division by zero\n",
    "        X_weighted = X_scaled * weights\n",
    "        adaptive_lasso = LassoCV(cv=5, random_state=42).fit(X_weighted, y)\n",
    "        selected_adaptive = feature_names[adaptive_lasso.coef_ != 0]\n",
    "        results[\"AdaptiveLasso\"] = list(selected_adaptive)\n",
    "        if verbose:\n",
    "            print(f\"\\n🔹 Adaptive Lasso selected {len(selected_adaptive)} features:\\n{list(selected_adaptive)}\")\n",
    "    except Exception as e:\n",
    "        results[\"AdaptiveLasso\"] = []\n",
    "        if verbose:\n",
    "            print(f\"\\n⚠️ Adaptive Lasso failed: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "group_map = [i // 2 for i in range(df.shape[1])]\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "feature_results = step25_regularization_feature_selection(df, y, group_map=group_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46e26c51-af23-40eb-9964-97254b6bd783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.0.2)\n",
      "Requirement already satisfied: lightgbm in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.6.0)\n",
      "Requirement already satisfied: shap in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.48.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from xgboost) (2.2.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from xgboost) (1.15.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from shap) (2.3.0)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from shap) (4.67.1)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from shap) (25.0)\n",
      "Requirement already satisfied: slicer==0.0.8 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba>=0.54 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from shap) (0.61.2)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from shap) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from shap) (4.14.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from numba>=0.54->shap) (0.44.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27.0->shap) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->shap) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->shap) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->shap) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🌳 Step 26: Tree-Based Feature Importance Methods\n",
      "\n",
      "🔹 Random Forest Top 10 Features:\n",
      " [('Student_ID Attendance (%)', np.float64(0.10638297872340426)), ('Student_ID', np.float64(0.09574468085106383)), ('Attendance (%)_zscore', np.float64(0.09574468085106383)), ('Marks', np.float64(0.0851063829787234)), ('Student_ID_zscore', np.float64(0.0851063829787234)), ('Student_ID Marks', np.float64(0.07446808510638298)), ('Student_ID Marks_zscore', np.float64(0.07446808510638298)), ('pca_1', np.float64(0.07446808510638298)), ('Attendance (%)', np.float64(0.06382978723404255)), ('Marks Attendance (%)', np.float64(0.06382978723404255))]\n",
      "\n",
      "🔹 XGBoost Top 10 Features:\n",
      " [('Student_ID', np.float32(1.0)), ('Marks', np.float32(0.0)), ('Attendance (%)', np.float32(0.0)), ('Student_ID Marks', np.float32(0.0)), ('Student_ID Attendance (%)', np.float32(0.0)), ('Marks Attendance (%)', np.float32(0.0)), ('Student_ID_zscore', np.float32(0.0)), ('Marks_zscore', np.float32(0.0)), ('Attendance (%)_zscore', np.float32(0.0)), ('Student_ID Marks_zscore', np.float32(0.0))]\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 5, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 0.600000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "\n",
      "🔹 LightGBM Top 10 Features:\n",
      " [('Student_ID', np.int32(0)), ('Marks', np.int32(0)), ('Attendance (%)', np.int32(0)), ('Student_ID Marks', np.int32(0)), ('Student_ID Attendance (%)', np.int32(0)), ('Marks Attendance (%)', np.int32(0)), ('Student_ID_zscore', np.int32(0)), ('Marks_zscore', np.int32(0)), ('Attendance (%)_zscore', np.int32(0)), ('Student_ID Marks_zscore', np.int32(0))]\n",
      "\n",
      "🔹 Permutation Importance Top 10 Features:\n",
      " [('Student_ID', np.float64(0.0)), ('Marks', np.float64(0.0)), ('Attendance (%)', np.float64(0.0)), ('Student_ID Marks', np.float64(0.0)), ('Student_ID Attendance (%)', np.float64(0.0)), ('Marks Attendance (%)', np.float64(0.0)), ('Student_ID_zscore', np.float64(0.0)), ('Marks_zscore', np.float64(0.0)), ('Attendance (%)_zscore', np.float64(0.0)), ('Student_ID Marks_zscore', np.float64(0.0))]\n",
      "\n",
      "🔹 SHAP Analysis (may take a moment)...\n",
      "🔹 SHAP Top 10 Features:\n",
      " [('Attendance (%)_zscore', np.float64(0.04319999903440476)), ('Student_ID Attendance (%)', np.float64(0.03999999910593033)), ('Marks', np.float64(0.03839999914169311)), ('Student_ID', np.float64(0.0367999991774559)), ('Student_ID_zscore', np.float64(0.034399999231100084)), ('Student_ID Marks', np.float64(0.03359999924898148)), ('Student_ID Marks_zscore', np.float64(0.03359999924898148)), ('pca_1', np.float64(0.03359999924898148)), ('Marks Attendance (%)', np.float64(0.028799999356269833)), ('Attendance (%)', np.float64(0.027199999392032624))]\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost lightgbm shap scikit-learn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def step26_tree_based_feature_importance(X, y, task='regression', top_n=10, random_state=42):\n",
    "    \"\"\"\n",
    "    Step 26: Tree-Based Feature Importance\n",
    "\n",
    "    Parameters:\n",
    "    - X: pd.DataFrame, features\n",
    "    - y: pd.Series or np.array, target\n",
    "    - task: 'regression' or 'classification'\n",
    "    - top_n: number of top features to show\n",
    "    - random_state: for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    - feature_scores: dict with method -> sorted list of feature importances\n",
    "    \"\"\"\n",
    "    print(\"\\n🌳 Step 26: Tree-Based Feature Importance Methods\\n\")\n",
    "\n",
    "    feature_scores = {}\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state, test_size=0.2)\n",
    "    feature_names = X.columns\n",
    "\n",
    "    # -------------------- 1️⃣ Random Forest --------------------\n",
    "    rf_model = RandomForestRegressor(random_state=random_state)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    rf_importances = rf_model.feature_importances_\n",
    "    rf_sorted = sorted(zip(feature_names, rf_importances), key=lambda x: x[1], reverse=True)\n",
    "    feature_scores['RandomForest'] = rf_sorted\n",
    "    print(f\"🔹 Random Forest Top {top_n} Features:\\n\", rf_sorted[:top_n])\n",
    "\n",
    "    # -------------------- 2️⃣ XGBoost --------------------\n",
    "    xgb_model = XGBRegressor(random_state=random_state, verbosity=0)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    xgb_importances = xgb_model.feature_importances_\n",
    "    xgb_sorted = sorted(zip(feature_names, xgb_importances), key=lambda x: x[1], reverse=True)\n",
    "    feature_scores['XGBoost'] = xgb_sorted\n",
    "    print(f\"\\n🔹 XGBoost Top {top_n} Features:\\n\", xgb_sorted[:top_n])\n",
    "\n",
    "    # -------------------- 3️⃣ LightGBM --------------------\n",
    "    lgb_model = LGBMRegressor(random_state=random_state)\n",
    "    lgb_model.fit(X_train, y_train)\n",
    "    lgb_importances = lgb_model.feature_importances_\n",
    "    lgb_sorted = sorted(zip(feature_names, lgb_importances), key=lambda x: x[1], reverse=True)\n",
    "    feature_scores['LightGBM'] = lgb_sorted\n",
    "    print(f\"\\n🔹 LightGBM Top {top_n} Features:\\n\", lgb_sorted[:top_n])\n",
    "\n",
    "    # -------------------- 4️⃣ Permutation Importance --------------------\n",
    "    perm_result = permutation_importance(rf_model, X_test, y_test, n_repeats=10, random_state=random_state)\n",
    "    perm_sorted = sorted(zip(feature_names, perm_result.importances_mean), key=lambda x: x[1], reverse=True)\n",
    "    feature_scores['Permutation'] = perm_sorted\n",
    "    print(f\"\\n🔹 Permutation Importance Top {top_n} Features:\\n\", perm_sorted[:top_n])\n",
    "\n",
    "    # -------------------- 5️⃣ SHAP --------------------\n",
    "    print(\"\\n🔹 SHAP Analysis (may take a moment)...\")\n",
    "    explainer = shap.Explainer(rf_model, X_train)\n",
    "    shap_values = explainer(X_train)\n",
    "    shap_importances = np.abs(shap_values.values).mean(axis=0)\n",
    "    shap_sorted = sorted(zip(feature_names, shap_importances), key=lambda x: x[1], reverse=True)\n",
    "    feature_scores['SHAP'] = shap_sorted\n",
    "    print(f\"🔹 SHAP Top {top_n} Features:\\n\", shap_sorted[:top_n])\n",
    "\n",
    "    return feature_scores\n",
    "\n",
    "X=df;\n",
    "results = step26_tree_based_feature_importance(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d8d7b5a2-0887-4c6e-adfd-6820b226aead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==2.2.0\n",
      "  Downloading numpy-2.2.0-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "     ------ --------------------------------- 10.2/60.8 kB ? eta -:--:--\n",
      "     ------------------------------- ------ 51.2/60.8 kB 525.1 kB/s eta 0:00:01\n",
      "     -------------------------------------- 60.8/60.8 kB 538.3 kB/s eta 0:00:00\n",
      "Downloading numpy-2.2.0-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/12.9 MB 7.3 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.4/12.9 MB 15.3 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 2.1/12.9 MB 16.6 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 2.1/12.9 MB 16.6 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 2.1/12.9 MB 16.6 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 2.1/12.9 MB 16.6 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 2.1/12.9 MB 16.6 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 3.1/12.9 MB 8.7 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 3.1/12.9 MB 8.7 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 3.1/12.9 MB 8.7 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 3.1/12.9 MB 8.7 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 3.1/12.9 MB 8.7 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 4.1/12.9 MB 7.0 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 5.1/12.9 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 5.2/12.9 MB 8.0 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 5.2/12.9 MB 8.0 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 5.2/12.9 MB 8.0 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 5.7/12.9 MB 6.7 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.5/12.9 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 7.3/12.9 MB 7.8 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 8.4/12.9 MB 8.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 8.4/12.9 MB 8.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 8.4/12.9 MB 8.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 8.4/12.9 MB 8.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 8.4/12.9 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 9.1/12.9 MB 7.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.8/12.9 MB 7.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.6/12.9 MB 8.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.4/12.9 MB 8.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.8/12.9 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.9/12.9 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.9/12.9 MB 9.2 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.6\n",
      "    Uninstalling numpy-2.2.6:\n",
      "      Successfully uninstalled numpy-2.2.6\n",
      "Successfully installed numpy-2.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install numpy==2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c3128b-d181-4fa2-9524-293b1d85c229",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
